[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nIdris Huang\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nIdris Huang\n\n\nApr 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/HW1/hw1_questions.html",
    "href": "blog/HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan and John List (2007) carried out a natural field experiment to investigate the effectiveness of different matching grant rates (1:1, 2:1, and 3:1) in charitable fundraising. In this work, I replicated and discussed some key results:\n\nChecking randomization balance\n\nEstimating the effect of any matching grant on donation probability\n\nComparing different match ratios\n\nExamining the size of donations\n\nDemonstrating the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) with simulations\n\n\n\nShow/Hide Code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\ndf_treat = df[df['treatment'] == 1]\ndf_cont  = df[df['control'] == 1]"
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#introduction",
    "href": "blog/HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan and John List (2007) carried out a natural field experiment to investigate the effectiveness of different matching grant rates (1:1, 2:1, and 3:1) in charitable fundraising. In this work, I replicated and discussed some key results:\n\nChecking randomization balance\n\nEstimating the effect of any matching grant on donation probability\n\nComparing different match ratios\n\nExamining the size of donations\n\nDemonstrating the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) with simulations\n\n\n\nShow/Hide Code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\ndf_treat = df[df['treatment'] == 1]\ndf_cont  = df[df['control'] == 1]"
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#data",
    "href": "blog/HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\n\nShow/Hide Code\nprint(\"Columns:\\n\", df.columns.tolist())\nprint(\"\\nSummary statistics:\\n\", df.describe())\n\n\nColumns:\n ['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25', 'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1', 'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra', 'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple', 'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse', 'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush', 'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack', 'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba', 'pop_propurban']\n\nSummary statistics:\n           treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168561      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378115     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258654  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nInterpretation: About 50,000 observations; key columns include treatment, control, gave, amount, etc."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#balance-test",
    "href": "blog/HW1/hw1_questions.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Test",
    "text": "Balance Test\n\n\nShow/Hide Code\nmrm2_treat = df_treat['mrm2'].dropna()\nmrm2_cont  = df_cont['mrm2'].dropna()\n\nt_stat, p_val, df_deg = ttest_ind(mrm2_treat, mrm2_cont, usevar='unequal')\nprint(f\"T-test for mrm2: t={t_stat:.4f}, p={p_val:.4g}, df={df_deg:.1f}\")\n\nmodel_bal = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nshort_summary(model_bal, label=\"OLS for mrm2 on treatment\")\n\n\nT-test for mrm2: t=0.1195, p=0.9049, df=33394.5\n=== OLS for mrm2 on treatment ===\n                coef   std err           t     P&gt;|t|\nIntercept  12.998142  0.093526  138.978873  0.000000\ntreatment   0.013686  0.114534    0.119492  0.904886\n\n\n\nResult: No significant difference in mrm2, suggesting balance."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#experimental-results",
    "href": "blog/HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\n1. Charitable Contribution Made\n\nA. Bar Plot\n\n\nShow/Hide Code\ngave_treat_mean = df_treat['gave'].mean()\ngave_cont_mean  = df_cont['gave'].mean()\nprint(\"Proportion gave - Control:\", gave_cont_mean, \"Treatment:\", gave_treat_mean)\n\n\nProportion gave - Control: 0.017858212980164198 Treatment: 0.02203856749311295\n\n\n\n\n\nBarplot_Proportion_Donors\n\n\n\n\nB. T-test & Regression\n\n\nShow/Hide Code\ngave_treat_vals = df_treat['gave']\ngave_cont_vals  = df_cont['gave']\nt_gave, p_gave, df_g = ttest_ind(gave_treat_vals, gave_cont_vals, usevar='unequal')\nprint(f\"gave T-test: t={t_gave:.4f}, p={p_gave:.4g}, df={df_g:.1f}\")\n\nmodel_gave_ols = smf.ols(\"gave ~ treatment\", data=df).fit()\nshort_summary(model_gave_ols, label=\"OLS for gave\")\n\nmodel_gave_probit = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nshort_summary(model_gave_probit, label=\"Probit for gave\")\n\n\ngave T-test: t=3.2095, p=0.001331, df=36576.8\n=== OLS for gave ===\n               coef   std err          t         P&gt;|t|\nIntercept  0.017858  0.001101  16.224643  4.779032e-59\ntreatment  0.004180  0.001348   3.101361  1.927403e-03\n\n=== Probit for gave ===\n               coef   std err         t     P&gt;|t|\nIntercept -2.100141  0.023316 -90.07277  0.000000\ntreatment  0.086785  0.027879   3.11293  0.001852\n\n\n\nResult: Matching grants significantly increase donation probability, though the effect size is small.\n\n\n\n2. Differences Between Match Rates\n\n\nShow/Hide Code\ngave_1to1 = df[df['ratio'] == 1]['gave']\ngave_2to1 = df[df['ratio2'] == 1]['gave']\ngave_3to1 = df[df['ratio3'] == 1]['gave']\n\nt_12, p_12, df_12 = ttest_ind(gave_1to1, gave_2to1, usevar='unequal')\nt_23, p_23, df_23 = ttest_ind(gave_2to1, gave_3to1, usevar='unequal')\nprint(f\"1:1 vs 2:1 =&gt; t={t_12:.4f}, p={p_12:.4g}\")\nprint(f\"2:1 vs 3:1 =&gt; t={t_23:.4f}, p={p_23:.4g}\")\n\nmodel_ratio = smf.ols(\"gave ~ ratio + ratio2 + ratio3\", data=df).fit()\nshort_summary(model_ratio, label=\"OLS for gave on ratio dummies\")\n\nr_1 = gave_1to1.mean()\nr_2 = gave_2to1.mean()\nr_3 = gave_3to1.mean()\nprint(\"Diff(1:1 vs 2:1):\", r_2 - r_1)\nprint(\"Diff(2:1 vs 3:1):\", r_3 - r_2)\n\n\n1:1 vs 2:1 =&gt; t=-0.9650, p=0.3345\n2:1 vs 3:1 =&gt; t=-0.0501, p=0.96\n=== OLS for gave on ratio dummies ===\n                    coef       std err          t         P&gt;|t|\nIntercept   1.802911e-02  1.386416e-03  13.004112  1.339110e-38\nratio[T.1]  2.732484e-03  1.914416e-03   1.427320  1.534940e-01\nratio[T.2] -3.535509e+10  2.437258e+11  -0.145061  8.846633e-01\nratio[T.3] -1.520186e+09  1.038114e+10  -0.146437  8.835767e-01\nratio2      3.535509e+10  2.437258e+11   0.145061  8.846633e-01\nratio3      1.520186e+09  1.038114e+10   0.146437  8.835767e-01\n\nDiff(1:1 vs 2:1): 0.0018842510217149944\nDiff(2:1 vs 3:1): 0.00010002398025293902\n\n\nResult: No evidence that higher match ratios lead to significantly greater giving.\n\n\n3. Size of Charitable Contribution\n\nA. Unconditional and Conditional\n\n\nShow/Hide Code\namt_treat = df_treat['amount'].fillna(0)\namt_cont  = df_cont['amount'].fillna(0)\nt_amt, p_amt, _ = ttest_ind(amt_treat, amt_cont, usevar='unequal')\nprint(f\"Uncond. amount T-test: t={t_amt:.4f}, p={p_amt:.4g}\")\n\nmodel_amt_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nshort_summary(model_amt_ols, label=\"OLS for unconditional 'amount'\")\n\ndf_givers = df[df['gave'] == 1]\namt_treat_g = df_givers[df_givers['treatment']==1]['amount']\namt_cont_g  = df_givers[df_givers['control'] ==1]['amount']\n\nt_amt_g, p_amt_g, _ = ttest_ind(amt_treat_g, amt_cont_g, usevar='unequal')\nprint(f\"Cond. amount T-test: t={t_amt_g:.4f}, p={p_amt_g:.4g}\")\n\nmodel_amt_cond_ols = smf.ols(\"amount ~ treatment\", data=df_givers).fit()\nshort_summary(model_amt_cond_ols, label=\"OLS for amount among donors\")\n\n\nUncond. amount T-test: t=1.9183, p=0.05509\n=== OLS for unconditional 'amount' ===\n               coef   std err          t         P&gt;|t|\nIntercept  0.813268  0.067418  12.062995  1.843438e-33\ntreatment  0.153605  0.082561   1.860503  6.282029e-02\n\nCond. amount T-test: t=-0.5846, p=0.559\n=== OLS for amount among donors ===\n                coef   std err          t         P&gt;|t|\nIntercept  45.540268  2.423378  18.792063  5.473578e-68\ntreatment  -1.668393  2.872384  -0.580839  5.614756e-01\n\n\n\nResult: Very small/unreliable difference in donation amounts.\n\n\nB. Histograms\n\n\nInterpretation: Distributions appear similar between groups."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#simulation-experiments",
    "href": "blog/HW1/hw1_questions.html#simulation-experiments",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiments",
    "text": "Simulation Experiments\n\nLaw of Large Numbers\n\n\nShow/Hide Code\nN_sims = 10000\np_c = 0.018\np_t = 0.022\n\nsim_c = np.random.binomial(1, p_c, N_sims)\nsim_t = np.random.binomial(1, p_t, N_sims)\ndiff_vec = sim_t - sim_c\ncum_avg = np.cumsum(diff_vec)/np.arange(1,N_sims+1)\n\n\n\n\n\nCumulative Average difference plot\n\n\nInterpretation: Cumulative average converges near 0.004 (the true difference in proportions).\n\n\nCentral Limit Theorem\n\n\nShow/Hide Code\ndef draw_mean_diffs(n_control, n_treatment, iters=1000):\n    out = []\n    for _ in range(iters):\n        c_draws = np.random.binomial(1, p_c, n_control)\n        t_draws = np.random.binomial(1, p_t, n_treatment)\n        out.append(t_draws.mean() - c_draws.mean())\n    return np.array(out)\n\nfor s in [50, 200, 500, 1000]:\n    diffs = draw_mean_diffs(s, s)\n\n\n\n\n\n\nInterpretation: As n increases, the sampling distribution becomes tighter and more bell-shaped."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#conclusion",
    "href": "blog/HW1/hw1_questions.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThe results replicate those of Karlan and List (2007): matching grants increase donation rates, but higher match ratios do not improve outcomes further. The average donation amount is unaffected. Simulations illustrate the LLN and CLT in practice."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#full-code",
    "href": "blog/HW1/hw1_questions.html#full-code",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Full Code",
    "text": "Full Code\n\n\nShow/Hide Code\n# A Replication of Karlan and List (2007)\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\n\n# Balance Test\ndf_treat = df[df['treatment'] == 1]\ndf_cont = df[df['control'] == 1]\n\nmrm2_treat = df_treat['mrm2'].dropna()\nmrm2_cont = df_cont['mrm2'].dropna()\nt_stat, p_val, df_deg = ttest_ind(mrm2_treat, mrm2_cont, usevar='unequal')\nmodel_bal = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nshort_summary(model_bal, label=\"OLS for mrm2 on treatment\")\n\n# Charitable Contributions\ngave_treat_mean = df_treat['gave'].mean()\ngave_cont_mean = df_cont['gave'].mean()\nplt.bar([\"Control\", \"Treatment\"], [gave_cont_mean, gave_treat_mean], color=[\"#1f77b4\",\"#ff7f0e\"])\nplt.title(\"Proportion Who Donated: Control vs Treatment\")\nplt.ylabel(\"Proportion\")\nplt.show()\n\ngave_treat_vals = df_treat['gave']\ngave_cont_vals = df_cont['gave']\nt_gave, p_gave, df_g = ttest_ind(gave_treat_vals, gave_cont_vals, usevar='unequal')\nmodel_gave_ols = smf.ols(\"gave ~ treatment\", data=df).fit()\nshort_summary(model_gave_ols, label=\"OLS for gave on treatment\")\nmodel_gave_probit = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nshort_summary(model_gave_probit, label=\"Probit for gave on treatment\")\n\n# Match Ratios\ngave_1to1 = df[df['ratio'] == 1]['gave']\ngave_2to1 = df[df['ratio2'] == 1]['gave']\ngave_3to1 = df[df['ratio3'] == 1]['gave']\nt_12, p_12, df_12 = ttest_ind(gave_1to1, gave_2to1, usevar='unequal')\nt_23, p_23, df_23 = ttest_ind(gave_2to1, gave_3to1, usevar='unequal')\nmodel_ratio = smf.ols(\"gave ~ ratio + ratio2 + ratio3\", data=df).fit()\nshort_summary(model_ratio, label=\"OLS for gave on ratio dummies\")\nprint(\"Response rate difference, 1:1 vs 2:1 =\", (gave_2to1.mean() - gave_1to1.mean())*100)\nprint(\"Response rate difference, 2:1 vs 3:1 =\", (gave_3to1.mean() - gave_2to1.mean())*100)\n\n# Donation Amounts\namt_treat = df_treat['amount'].fillna(0)\namt_cont = df_cont['amount'].fillna(0)\nt_amt, p_amt, df_amt_ = ttest_ind(amt_treat, amt_cont, usevar='unequal')\nmodel_amt_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nshort_summary(model_amt_ols, label=\"OLS for unconditional 'amount'\")\n\ndf_givers = df[df['gave'] == 1]\namt_treat_g = df_givers[df_givers['treatment']==1]['amount']\namt_cont_g = df_givers[df_givers['control']==1]['amount']\nt_amt_g, p_amt_g, df_amt_g = ttest_ind(amt_treat_g, amt_cont_g, usevar='unequal')\nmodel_amt_cond_ols = smf.ols(\"amount ~ treatment\", data=df_givers).fit()\nshort_summary(model_amt_cond_ols, label=\"OLS for amount among donors\")\n\n# Histograms\nmean_treat_g = amt_treat_g.mean()\nmean_cont_g = amt_cont_g.mean()\n\nplt.hist(amt_treat_g, bins=30, color=\"#9467bd\")\nplt.axvline(mean_treat_g, color='red', linestyle='--', linewidth=2)\nplt.title(\"Histogram: Donation Amounts (Treatment donors)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.hist(amt_cont_g, bins=30, color=\"#d62728\")\nplt.axvline(mean_cont_g, color='red', linestyle='--', linewidth=2)\nplt.title(\"Histogram: Donation Amounts (Control donors)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# LLN Simulation\nN_sims = 10000\np_c = 0.018\np_t = 0.022\nsim_c = np.random.binomial(1, p_c, size=N_sims)\nsim_t = np.random.binomial(1, p_t, size=N_sims)\ndiff_vec = sim_t - sim_c\ncum_avg = np.cumsum(diff_vec) / np.arange(1, N_sims+1)\n\nplt.plot(cum_avg, color=\"#2ca02c\")\nplt.axhline(y=(p_t - p_c), color='red', linestyle='--')\nplt.title(\"Cumulative Average of Differences (Treatment - Control)\")\nplt.xlabel(\"Number of draws\")\nplt.ylabel(\"Cumulative average difference\")\nplt.show()\n\n# CLT Simulation\ndef draw_mean_diffs(n_control, n_treatment, iters=1000):\n    out = []\n    for _ in range(iters):\n        c_draws = np.random.binomial(1, p_c, size=n_control)\n        t_draws = np.random.binomial(1, p_t, size=n_treatment)\n        out.append(t_draws.mean() - c_draws.mean())\n    return np.array(out)\n\nfor s in [50, 200, 500, 1000]:\n    diffs_s = draw_mean_diffs(s, s)\n    plt.hist(diffs_s, bins=30, color=\"#ff7f0e\", alpha=0.8)\n    plt.axvline(0, color='black', linestyle='--')\n    plt.title(f\"Histogram of Differences (n={s}, 1000 reps)\")\n    plt.xlabel(\"Difference in means (treat - control)\")\n    plt.ylabel(\"Frequency\")\n    plt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Idris Huang",
    "section": "",
    "text": "Here is a paragraph about me!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project 2/index.html",
    "href": "blog/project 2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data"
  },
  {
    "objectID": "blog/project 2/index.html#section-1-data",
    "href": "blog/project 2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project 2/index.html#section-2-analysis",
    "href": "blog/project 2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/HW1/index.html",
    "href": "blog/HW1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan and John List (2007) carried out a natural field experiment to investigate the effectiveness of different matching grant rates (1:1, 2:1, and 3:1) in charitable fundraising. In this work, I replicated and discussed some key results:\n\nChecking randomization balance\n\nEstimating the effect of any matching grant on donation probability\n\nComparing different match ratios\n\nExamining the size of donations\n\nDemonstrating the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) with simulations\n\n\n\nShow/Hide Code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\ndf_treat = df[df['treatment'] == 1]\ndf_cont  = df[df['control'] == 1]"
  },
  {
    "objectID": "blog/HW1/index.html#introduction",
    "href": "blog/HW1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan and John List (2007) carried out a natural field experiment to investigate the effectiveness of different matching grant rates (1:1, 2:1, and 3:1) in charitable fundraising. In this work, I replicated and discussed some key results:\n\nChecking randomization balance\n\nEstimating the effect of any matching grant on donation probability\n\nComparing different match ratios\n\nExamining the size of donations\n\nDemonstrating the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) with simulations\n\n\n\nShow/Hide Code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\ndf_treat = df[df['treatment'] == 1]\ndf_cont  = df[df['control'] == 1]"
  },
  {
    "objectID": "blog/HW1/index.html#data",
    "href": "blog/HW1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\n\nShow/Hide Code\nprint(\"Columns:\\n\", df.columns.tolist())\nprint(\"\\nSummary statistics:\\n\", df.describe())\n\n\nColumns:\n ['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25', 'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1', 'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra', 'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple', 'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse', 'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush', 'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack', 'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba', 'pop_propurban']\n\nSummary statistics:\n           treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168561      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378115     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258654  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nInterpretation: About 50,000 observations; key columns include treatment, control, gave, amount, etc."
  },
  {
    "objectID": "blog/HW1/index.html#balance-test",
    "href": "blog/HW1/index.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Test",
    "text": "Balance Test\n\n\nShow/Hide Code\nmrm2_treat = df_treat['mrm2'].dropna()\nmrm2_cont  = df_cont['mrm2'].dropna()\n\nt_stat, p_val, df_deg = ttest_ind(mrm2_treat, mrm2_cont, usevar='unequal')\nprint(f\"T-test for mrm2: t={t_stat:.4f}, p={p_val:.4g}, df={df_deg:.1f}\")\n\nmodel_bal = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nshort_summary(model_bal, label=\"OLS for mrm2 on treatment\")\n\n\nT-test for mrm2: t=0.1195, p=0.9049, df=33394.5\n=== OLS for mrm2 on treatment ===\n                coef   std err           t     P&gt;|t|\nIntercept  12.998142  0.093526  138.978873  0.000000\ntreatment   0.013686  0.114534    0.119492  0.904886\n\n\n\nResult: No significant difference in mrm2, suggesting balance."
  },
  {
    "objectID": "blog/HW1/index.html#experimental-results",
    "href": "blog/HW1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\n1. Charitable Contribution Made\n\nA. Bar Plot\n\n\nShow/Hide Code\ngave_treat_mean = df_treat['gave'].mean()\ngave_cont_mean  = df_cont['gave'].mean()\nprint(\"Proportion gave - Control:\", gave_cont_mean, \"Treatment:\", gave_treat_mean)\n\n\nProportion gave - Control: 0.017858212980164198 Treatment: 0.02203856749311295\n\n\n\n\n\nBarplot_Proportion_Donors\n\n\n\n\nB. T-test & Regression\n\n\nShow/Hide Code\ngave_treat_vals = df_treat['gave']\ngave_cont_vals  = df_cont['gave']\nt_gave, p_gave, df_g = ttest_ind(gave_treat_vals, gave_cont_vals, usevar='unequal')\nprint(f\"gave T-test: t={t_gave:.4f}, p={p_gave:.4g}, df={df_g:.1f}\")\n\nmodel_gave_ols = smf.ols(\"gave ~ treatment\", data=df).fit()\nshort_summary(model_gave_ols, label=\"OLS for gave\")\n\nmodel_gave_probit = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nshort_summary(model_gave_probit, label=\"Probit for gave\")\n\n\ngave T-test: t=3.2095, p=0.001331, df=36576.8\n=== OLS for gave ===\n               coef   std err          t         P&gt;|t|\nIntercept  0.017858  0.001101  16.224643  4.779032e-59\ntreatment  0.004180  0.001348   3.101361  1.927403e-03\n\n=== Probit for gave ===\n               coef   std err         t     P&gt;|t|\nIntercept -2.100141  0.023316 -90.07277  0.000000\ntreatment  0.086785  0.027879   3.11293  0.001852\n\n\n\nResult: Matching grants significantly increase donation probability, though the effect size is small.\n\n\n\n2. Differences Between Match Rates\n\n\nShow/Hide Code\ngave_1to1 = df[df['ratio'] == 1]['gave']\ngave_2to1 = df[df['ratio2'] == 1]['gave']\ngave_3to1 = df[df['ratio3'] == 1]['gave']\n\nt_12, p_12, df_12 = ttest_ind(gave_1to1, gave_2to1, usevar='unequal')\nt_23, p_23, df_23 = ttest_ind(gave_2to1, gave_3to1, usevar='unequal')\nprint(f\"1:1 vs 2:1 =&gt; t={t_12:.4f}, p={p_12:.4g}\")\nprint(f\"2:1 vs 3:1 =&gt; t={t_23:.4f}, p={p_23:.4g}\")\n\nmodel_ratio = smf.ols(\"gave ~ ratio + ratio2 + ratio3\", data=df).fit()\nshort_summary(model_ratio, label=\"OLS for gave on ratio dummies\")\n\nr_1 = gave_1to1.mean()\nr_2 = gave_2to1.mean()\nr_3 = gave_3to1.mean()\nprint(\"Diff(1:1 vs 2:1):\", r_2 - r_1)\nprint(\"Diff(2:1 vs 3:1):\", r_3 - r_2)\n\n\n1:1 vs 2:1 =&gt; t=-0.9650, p=0.3345\n2:1 vs 3:1 =&gt; t=-0.0501, p=0.96\n=== OLS for gave on ratio dummies ===\n                    coef       std err          t         P&gt;|t|\nIntercept   1.802911e-02  1.386416e-03  13.004112  1.339110e-38\nratio[T.1]  2.732484e-03  1.914416e-03   1.427320  1.534940e-01\nratio[T.2] -3.535509e+10  2.437258e+11  -0.145061  8.846633e-01\nratio[T.3] -1.520186e+09  1.038114e+10  -0.146437  8.835767e-01\nratio2      3.535509e+10  2.437258e+11   0.145061  8.846633e-01\nratio3      1.520186e+09  1.038114e+10   0.146437  8.835767e-01\n\nDiff(1:1 vs 2:1): 0.0018842510217149944\nDiff(2:1 vs 3:1): 0.00010002398025293902\n\n\nResult: No evidence that higher match ratios lead to significantly greater giving.\n\n\n3. Size of Charitable Contribution\n\nA. Unconditional and Conditional\n\n\nShow/Hide Code\namt_treat = df_treat['amount'].fillna(0)\namt_cont  = df_cont['amount'].fillna(0)\nt_amt, p_amt, _ = ttest_ind(amt_treat, amt_cont, usevar='unequal')\nprint(f\"Uncond. amount T-test: t={t_amt:.4f}, p={p_amt:.4g}\")\n\nmodel_amt_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nshort_summary(model_amt_ols, label=\"OLS for unconditional 'amount'\")\n\ndf_givers = df[df['gave'] == 1]\namt_treat_g = df_givers[df_givers['treatment']==1]['amount']\namt_cont_g  = df_givers[df_givers['control'] ==1]['amount']\n\nt_amt_g, p_amt_g, _ = ttest_ind(amt_treat_g, amt_cont_g, usevar='unequal')\nprint(f\"Cond. amount T-test: t={t_amt_g:.4f}, p={p_amt_g:.4g}\")\n\nmodel_amt_cond_ols = smf.ols(\"amount ~ treatment\", data=df_givers).fit()\nshort_summary(model_amt_cond_ols, label=\"OLS for amount among donors\")\n\n\nUncond. amount T-test: t=1.9183, p=0.05509\n=== OLS for unconditional 'amount' ===\n               coef   std err          t         P&gt;|t|\nIntercept  0.813268  0.067418  12.062995  1.843438e-33\ntreatment  0.153605  0.082561   1.860503  6.282029e-02\n\nCond. amount T-test: t=-0.5846, p=0.559\n=== OLS for amount among donors ===\n                coef   std err          t         P&gt;|t|\nIntercept  45.540268  2.423378  18.792063  5.473578e-68\ntreatment  -1.668393  2.872384  -0.580839  5.614756e-01\n\n\n\nResult: Very small/unreliable difference in donation amounts.\n\n\nB. Histograms\n\n\nInterpretation: Distributions appear similar between groups."
  },
  {
    "objectID": "blog/HW1/index.html#simulation-experiments",
    "href": "blog/HW1/index.html#simulation-experiments",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiments",
    "text": "Simulation Experiments\n\nLaw of Large Numbers\n\n\nShow/Hide Code\nN_sims = 10000\np_c = 0.018\np_t = 0.022\n\nsim_c = np.random.binomial(1, p_c, N_sims)\nsim_t = np.random.binomial(1, p_t, N_sims)\ndiff_vec = sim_t - sim_c\ncum_avg = np.cumsum(diff_vec)/np.arange(1,N_sims+1)\n\n\n\n\n\nCumulative Average difference plot\n\n\nInterpretation: Cumulative average converges near 0.004 (the true difference in proportions).\n\n\nCentral Limit Theorem\n\n\nShow/Hide Code\ndef draw_mean_diffs(n_control, n_treatment, iters=1000):\n    out = []\n    for _ in range(iters):\n        c_draws = np.random.binomial(1, p_c, n_control)\n        t_draws = np.random.binomial(1, p_t, n_treatment)\n        out.append(t_draws.mean() - c_draws.mean())\n    return np.array(out)\n\nfor s in [50, 200, 500, 1000]:\n    diffs = draw_mean_diffs(s, s)\n\n\n\n\n\n\nInterpretation: As n increases, the sampling distribution becomes tighter and more bell-shaped."
  },
  {
    "objectID": "blog/HW1/index.html#conclusion",
    "href": "blog/HW1/index.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThe results replicate those of Karlan and List (2007): matching grants increase donation rates, but higher match ratios do not improve outcomes further. The average donation amount is unaffected. Simulations illustrate the LLN and CLT in practice."
  },
  {
    "objectID": "blog/HW1/index.html#full-code",
    "href": "blog/HW1/index.html#full-code",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Full Code",
    "text": "Full Code\n\n\nShow/Hide Code\n# A Replication of Karlan and List (2007)\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\n\n# Balance Test\ndf_treat = df[df['treatment'] == 1]\ndf_cont = df[df['control'] == 1]\n\nmrm2_treat = df_treat['mrm2'].dropna()\nmrm2_cont = df_cont['mrm2'].dropna()\nt_stat, p_val, df_deg = ttest_ind(mrm2_treat, mrm2_cont, usevar='unequal')\nmodel_bal = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nshort_summary(model_bal, label=\"OLS for mrm2 on treatment\")\n\n# Charitable Contributions\ngave_treat_mean = df_treat['gave'].mean()\ngave_cont_mean = df_cont['gave'].mean()\nplt.bar([\"Control\", \"Treatment\"], [gave_cont_mean, gave_treat_mean], color=[\"#1f77b4\",\"#ff7f0e\"])\nplt.title(\"Proportion Who Donated: Control vs Treatment\")\nplt.ylabel(\"Proportion\")\nplt.show()\n\ngave_treat_vals = df_treat['gave']\ngave_cont_vals = df_cont['gave']\nt_gave, p_gave, df_g = ttest_ind(gave_treat_vals, gave_cont_vals, usevar='unequal')\nmodel_gave_ols = smf.ols(\"gave ~ treatment\", data=df).fit()\nshort_summary(model_gave_ols, label=\"OLS for gave on treatment\")\nmodel_gave_probit = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nshort_summary(model_gave_probit, label=\"Probit for gave on treatment\")\n\n# Match Ratios\ngave_1to1 = df[df['ratio'] == 1]['gave']\ngave_2to1 = df[df['ratio2'] == 1]['gave']\ngave_3to1 = df[df['ratio3'] == 1]['gave']\nt_12, p_12, df_12 = ttest_ind(gave_1to1, gave_2to1, usevar='unequal')\nt_23, p_23, df_23 = ttest_ind(gave_2to1, gave_3to1, usevar='unequal')\nmodel_ratio = smf.ols(\"gave ~ ratio + ratio2 + ratio3\", data=df).fit()\nshort_summary(model_ratio, label=\"OLS for gave on ratio dummies\")\nprint(\"Response rate difference, 1:1 vs 2:1 =\", (gave_2to1.mean() - gave_1to1.mean())*100)\nprint(\"Response rate difference, 2:1 vs 3:1 =\", (gave_3to1.mean() - gave_2to1.mean())*100)\n\n# Donation Amounts\namt_treat = df_treat['amount'].fillna(0)\namt_cont = df_cont['amount'].fillna(0)\nt_amt, p_amt, df_amt_ = ttest_ind(amt_treat, amt_cont, usevar='unequal')\nmodel_amt_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nshort_summary(model_amt_ols, label=\"OLS for unconditional 'amount'\")\n\ndf_givers = df[df['gave'] == 1]\namt_treat_g = df_givers[df_givers['treatment']==1]['amount']\namt_cont_g = df_givers[df_givers['control']==1]['amount']\nt_amt_g, p_amt_g, df_amt_g = ttest_ind(amt_treat_g, amt_cont_g, usevar='unequal')\nmodel_amt_cond_ols = smf.ols(\"amount ~ treatment\", data=df_givers).fit()\nshort_summary(model_amt_cond_ols, label=\"OLS for amount among donors\")\n\n# Histograms\nmean_treat_g = amt_treat_g.mean()\nmean_cont_g = amt_cont_g.mean()\n\nplt.hist(amt_treat_g, bins=30, color=\"#9467bd\")\nplt.axvline(mean_treat_g, color='red', linestyle='--', linewidth=2)\nplt.title(\"Histogram: Donation Amounts (Treatment donors)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.hist(amt_cont_g, bins=30, color=\"#d62728\")\nplt.axvline(mean_cont_g, color='red', linestyle='--', linewidth=2)\nplt.title(\"Histogram: Donation Amounts (Control donors)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# LLN Simulation\nN_sims = 10000\np_c = 0.018\np_t = 0.022\nsim_c = np.random.binomial(1, p_c, size=N_sims)\nsim_t = np.random.binomial(1, p_t, size=N_sims)\ndiff_vec = sim_t - sim_c\ncum_avg = np.cumsum(diff_vec) / np.arange(1, N_sims+1)\n\nplt.plot(cum_avg, color=\"#2ca02c\")\nplt.axhline(y=(p_t - p_c), color='red', linestyle='--')\nplt.title(\"Cumulative Average of Differences (Treatment - Control)\")\nplt.xlabel(\"Number of draws\")\nplt.ylabel(\"Cumulative average difference\")\nplt.show()\n\n# CLT Simulation\ndef draw_mean_diffs(n_control, n_treatment, iters=1000):\n    out = []\n    for _ in range(iters):\n        c_draws = np.random.binomial(1, p_c, size=n_control)\n        t_draws = np.random.binomial(1, p_t, size=n_treatment)\n        out.append(t_draws.mean() - c_draws.mean())\n    return np.array(out)\n\nfor s in [50, 200, 500, 1000]:\n    diffs_s = draw_mean_diffs(s, s)\n    plt.hist(diffs_s, bins=30, color=\"#ff7f0e\", alpha=0.8)\n    plt.axvline(0, color='black', linestyle='--')\n    plt.title(f\"Histogram of Differences (n={s}, 1000 reps)\")\n    plt.xlabel(\"Difference in means (treat - control)\")\n    plt.ylabel(\"Frequency\")\n    plt.show()"
  },
  {
    "objectID": "blog/HW2/index.html",
    "href": "blog/HW2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last five years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nShow/Hide Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\nimport pathlib\nimport pandas as pd, matplotlib.pyplot as plt, numpy as np, pathlib\nblue = (pd.read_csv(\"blueprinty.csv\")\n          .replace([np.inf, -np.inf], np.nan)\n          .dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"]))\nblue[\"age2\"] = blue[\"age\"]**2\n\n# summary tables\nprint(\"--- Patents by customer status ---\")\nprint(blue.groupby(\"iscustomer\")[\"patents\"].describe()[[\"mean\",\"std\"]])\nprint(\"\\n--- Age by customer status ---\")\nprint(blue.groupby(\"iscustomer\")[\"age\"].describe()[[\"mean\",\"std\",\"min\",\"max\"]])\nprint(\"\\n--- Region shares ---\")\nprint(pd.crosstab(blue[\"region\"], blue[\"iscustomer\"], normalize=\"columns\").round(3))\n\n\n--- Patents by customer status ---\n                mean       std\niscustomer                    \n0           3.473013  2.225060\n1           4.133056  2.546846\n\n--- Age by customer status ---\n                 mean       std   min   max\niscustomer                                 \n0           26.101570  6.945426   9.0  47.5\n1           26.900208  7.814678  10.0  49.0\n\n--- Region shares ---\niscustomer      0      1\nregion                  \nMidwest     0.184  0.077\nNortheast   0.268  0.682\nNorthwest   0.155  0.060\nSouth       0.153  0.073\nSouthwest   0.240  0.108\n\n\n\n\nShow/Hide Code\nfig, ax = plt.subplots()\nfor cust, grp in blue.groupby(\"iscustomer\"):\n    ax.hist(grp[\"patents\"],\n            bins=np.arange(grp[\"patents\"].max()+2)-0.5,\n            alpha=0.6, label=f\"Customer = {cust}\")\nax.set(title=\"Patent Count by Customer Status\",\n       xlabel=\"Patents (last 5 yrs)\", ylabel=\"Frequency\")\nax.legend()\npathlib.Path(\"figures\").mkdir(exist_ok=True)\nfig.savefig(\"figures/blue_hist_patents.png\", dpi=300)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last five years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nShow/Hide Code\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nY = blue[\"patents\"].astype(int).to_numpy()\n\ndef nll(lmbda):\n    l = lmbda[0]\n    return np.inf if l&lt;=0 else -(Y*np.log(l)-l-gammaln(Y+1)).sum()\n\nλ_grid = np.linspace(0.1, Y.mean()*3, 400)\nll_vals = [-nll([g]) for g in λ_grid]\n\nplt.figure()\nplt.plot(λ_grid, ll_vals)\nplt.axvline(Y.mean(), ls=\"--\", label=r\"$\\bar{Y}$\")\nplt.title(\"Log-likelihood for λ (Simple Poisson)\")\nplt.xlabel(\"λ\"); plt.ylabel(\"Log-likelihood\"); plt.legend()\nplt.tight_layout(); plt.savefig(\"figures/blue_ll_curve.png\", dpi=300)\n\nmle = minimize(nll, [Y.mean()], bounds=[(1e-9,None)])\nprint(\"MLE λ̂ =\", round(mle.x[0],4))\n\n\nMLE λ̂ = 3.6847\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that\n\\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i=\\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow/Hide Code\nimport statsmodels.formula.api as smf\nglm_blue = smf.glm(\"patents ~ age + age2 + C(region) + iscustomer\",\n                   data=blue, family=sm.families.Poisson()).fit()\nprint(glm_blue.summary2().tables[1][[\"Coef.\",\"Std.Err.\",\"z\",\"P&gt;|z|\"]].round(4))\n\n\n                         Coef.  Std.Err.        z   P&gt;|z|\nIntercept              -0.5089    0.1832  -2.7783  0.0055\nC(region)[T.Northeast]  0.0292    0.0436   0.6686  0.5037\nC(region)[T.Northwest] -0.0176    0.0538  -0.3268  0.7438\nC(region)[T.South]      0.0566    0.0527   1.0740  0.2828\nC(region)[T.Southwest]  0.0506    0.0472   1.0716  0.2839\nage                     0.1486    0.0139  10.7162  0.0000\nage2                   -0.0030    0.0003 -11.5132  0.0000\niscustomer              0.2076    0.0309   6.7192  0.0000\n\n\n\n\nShow/Hide Code\nmu_no  = glm_blue.predict(blue.assign(iscustomer=0))\nmu_yes = glm_blue.predict(blue.assign(iscustomer=1))\nprint(\"Average predicted Δ patents if all became customers:\",\n      round((mu_yes-mu_no).mean(),3))\n\n\nAverage predicted Δ patents if all became customers: 0.793\n\n\nInterpretation: The customer coefficient β̂ ≈ 0.208 (p &lt; 0.001) implies Blueprinty users file ≈ 23 % more patents; the average predicted gain is 0.79 patents over five years."
  },
  {
    "objectID": "blog/HW2/index.html#section-1-data",
    "href": "blog/HW2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/HW2/index.html#section-2-analysis",
    "href": "blog/HW2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "I analyzed the data."
  },
  {
    "objectID": "blog/HW2/index.html#blueprinty-case-study",
    "href": "blog/HW2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last five years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nShow/Hide Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\nimport pathlib\nimport pandas as pd, matplotlib.pyplot as plt, numpy as np, pathlib\nblue = (pd.read_csv(\"blueprinty.csv\")\n          .replace([np.inf, -np.inf], np.nan)\n          .dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"]))\nblue[\"age2\"] = blue[\"age\"]**2\n\n# summary tables\nprint(\"--- Patents by customer status ---\")\nprint(blue.groupby(\"iscustomer\")[\"patents\"].describe()[[\"mean\",\"std\"]])\nprint(\"\\n--- Age by customer status ---\")\nprint(blue.groupby(\"iscustomer\")[\"age\"].describe()[[\"mean\",\"std\",\"min\",\"max\"]])\nprint(\"\\n--- Region shares ---\")\nprint(pd.crosstab(blue[\"region\"], blue[\"iscustomer\"], normalize=\"columns\").round(3))\n\n\n--- Patents by customer status ---\n                mean       std\niscustomer                    \n0           3.473013  2.225060\n1           4.133056  2.546846\n\n--- Age by customer status ---\n                 mean       std   min   max\niscustomer                                 \n0           26.101570  6.945426   9.0  47.5\n1           26.900208  7.814678  10.0  49.0\n\n--- Region shares ---\niscustomer      0      1\nregion                  \nMidwest     0.184  0.077\nNortheast   0.268  0.682\nNorthwest   0.155  0.060\nSouth       0.153  0.073\nSouthwest   0.240  0.108\n\n\n\n\nShow/Hide Code\nfig, ax = plt.subplots()\nfor cust, grp in blue.groupby(\"iscustomer\"):\n    ax.hist(grp[\"patents\"],\n            bins=np.arange(grp[\"patents\"].max()+2)-0.5,\n            alpha=0.6, label=f\"Customer = {cust}\")\nax.set(title=\"Patent Count by Customer Status\",\n       xlabel=\"Patents (last 5 yrs)\", ylabel=\"Frequency\")\nax.legend()\npathlib.Path(\"figures\").mkdir(exist_ok=True)\nfig.savefig(\"figures/blue_hist_patents.png\", dpi=300)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last five years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nShow/Hide Code\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nY = blue[\"patents\"].astype(int).to_numpy()\n\ndef nll(lmbda):\n    l = lmbda[0]\n    return np.inf if l&lt;=0 else -(Y*np.log(l)-l-gammaln(Y+1)).sum()\n\nλ_grid = np.linspace(0.1, Y.mean()*3, 400)\nll_vals = [-nll([g]) for g in λ_grid]\n\nplt.figure()\nplt.plot(λ_grid, ll_vals)\nplt.axvline(Y.mean(), ls=\"--\", label=r\"$\\bar{Y}$\")\nplt.title(\"Log-likelihood for λ (Simple Poisson)\")\nplt.xlabel(\"λ\"); plt.ylabel(\"Log-likelihood\"); plt.legend()\nplt.tight_layout(); plt.savefig(\"figures/blue_ll_curve.png\", dpi=300)\n\nmle = minimize(nll, [Y.mean()], bounds=[(1e-9,None)])\nprint(\"MLE λ̂ =\", round(mle.x[0],4))\n\n\nMLE λ̂ = 3.6847\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that\n\\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i=\\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow/Hide Code\nimport statsmodels.formula.api as smf\nglm_blue = smf.glm(\"patents ~ age + age2 + C(region) + iscustomer\",\n                   data=blue, family=sm.families.Poisson()).fit()\nprint(glm_blue.summary2().tables[1][[\"Coef.\",\"Std.Err.\",\"z\",\"P&gt;|z|\"]].round(4))\n\n\n                         Coef.  Std.Err.        z   P&gt;|z|\nIntercept              -0.5089    0.1832  -2.7783  0.0055\nC(region)[T.Northeast]  0.0292    0.0436   0.6686  0.5037\nC(region)[T.Northwest] -0.0176    0.0538  -0.3268  0.7438\nC(region)[T.South]      0.0566    0.0527   1.0740  0.2828\nC(region)[T.Southwest]  0.0506    0.0472   1.0716  0.2839\nage                     0.1486    0.0139  10.7162  0.0000\nage2                   -0.0030    0.0003 -11.5132  0.0000\niscustomer              0.2076    0.0309   6.7192  0.0000\n\n\n\n\nShow/Hide Code\nmu_no  = glm_blue.predict(blue.assign(iscustomer=0))\nmu_yes = glm_blue.predict(blue.assign(iscustomer=1))\nprint(\"Average predicted Δ patents if all became customers:\",\n      round((mu_yes-mu_no).mean(),3))\n\n\nAverage predicted Δ patents if all became customers: 0.793\n\n\nInterpretation: The customer coefficient β̂ ≈ 0.208 (p &lt; 0.001) implies Blueprinty users file ≈ 23 % more patents; the average predicted gain is 0.79 patents over five years."
  },
  {
    "objectID": "blog/HW2/index.html#airbnb-case-study",
    "href": "blog/HW2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped 40 000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData\n\n\nShow/Hide Code\nair = (pd.read_csv(\"airbnb.csv\")\n         .assign(instant_bookable=lambda d:(d[\"instant_bookable\"]==\"t\").astype(int))\n         .dropna(subset=[\"number_of_reviews\",\"days\",\"room_type\",\"bathrooms\",\n                         \"bedrooms\",\"price\",\n                         \"review_scores_cleanliness\",\"review_scores_location\",\n                         \"review_scores_value\"]))\nprint(\"Rows after cleaning:\", len(air))\n\n\nRows after cleaning: 30160\n\n\n\n\nShow/Hide Code\nair[\"number_of_reviews\"].hist(bins=50)\nplt.title(\"Review Count Distribution\")\nplt.xlabel(\"Number of Reviews\"); plt.ylabel(\"Frequency\")\nplt.tight_layout(); plt.savefig(\"figures/air_reviews_hist.png\", dpi=300)\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation of Poisson Regression Model\n\n\nShow/Hide Code\nglm_air = smf.glm(\n    formula=\"\"\"number_of_reviews ~ days + bathrooms + bedrooms + price\n               + review_scores_cleanliness + review_scores_location\n               + review_scores_value + instant_bookable + C(room_type)\"\"\",\n    data=air, family=sm.families.Poisson()).fit()\n\nprint(glm_air.summary2().tables[1][[\"Coef.\",\"Std.Err.\",\"z\",\"P&gt;|z|\"]].round(4))\n\n\n                               Coef.  Std.Err.         z   P&gt;|z|\nIntercept                     3.4980    0.0161  217.3963  0.0000\nC(room_type)[T.Private room] -0.0105    0.0027   -3.8475  0.0001\nC(room_type)[T.Shared room]  -0.2463    0.0086  -28.5781  0.0000\ndays                          0.0001    0.0000  129.7553  0.0000\nbathrooms                    -0.1177    0.0037  -31.3942  0.0000\nbedrooms                      0.0741    0.0020   37.1972  0.0000\nprice                        -0.0000    0.0000   -2.1509  0.0315\nreview_scores_cleanliness     0.1131    0.0015   75.6106  0.0000\nreview_scores_location       -0.0769    0.0016  -47.7962  0.0000\nreview_scores_value          -0.0911    0.0018  -50.4899  0.0000\ninstant_bookable              0.3459    0.0029  119.6656  0.0000\n\n\nDiscussion: Older listings, lower prices, higher quality scores, and the instant-booking feature all raise expected bookings. Entire homes or apartments outperform private and shared rooms."
  },
  {
    "objectID": "blog/HW2/index.html#summary-conclusion",
    "href": "blog/HW2/index.html#summary-conclusion",
    "title": "Poisson Regression Examples",
    "section": "Summary & Conclusion",
    "text": "Summary & Conclusion\n\nBlueprinty: After controlling for age and region, customers secure ≈ 0.8 additional patents (≈ 23 % lift) over five years.\n\nAirBnB: Booking intensity (review count) responds strongly to listing age, price, room type, perceived quality, and instant-booking convenience.\n\nMaximum-Likelihood Poisson regression provides an effective framework for evaluating count-data outcomes in marketing analytics."
  },
  {
    "objectID": "blog/HW2/index.html#full-code",
    "href": "blog/HW2/index.html#full-code",
    "title": "Poisson Regression Examples",
    "section": "Full Code",
    "text": "Full Code\n\n\nShow/Hide Code\nimport pathlib, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nplt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=[\"#1f77b4\", \"#ff7f0e\"])\npathlib.Path(\"figures\").mkdir(exist_ok=True)\n\n#BLUEPRINTY CASE\nblue = (pd.read_csv(\"blueprinty.csv\")\n          .replace([np.inf, -np.inf], np.nan)\n          .dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"]))\nblue[\"age2\"] = blue[\"age\"] ** 2\nY_b = blue[\"patents\"].astype(int).values\n\n# Histograms and summary tables\nfig, ax = plt.subplots()\nfor cust, grp in blue.groupby(\"iscustomer\"):\n    ax.hist(grp[\"patents\"],\n            bins=np.arange(grp[\"patents\"].max() + 2) - 0.5,\n            alpha=0.6, label=f\"Customer = {cust}\")\nax.set(title=\"Patent count distribution by Blueprinty customer status\",\n       xlabel=\"Patents (last 5 yrs)\", ylabel=\"Frequency\")\nax.legend()\nfig.savefig(\"figures/blue_hist_patents.png\", dpi=300)\n\nprint(\"\\nMean ± SD of patents by customer status\")\nprint(blue.groupby(\"iscustomer\")[\"patents\"]\n           .agg([\"mean\", \"std\", \"count\"]).round(3))\n\nprint(\"\\nAge summary by customer status\")\nprint(blue.groupby(\"iscustomer\")[\"age\"]\n           .describe()[[\"mean\", \"std\", \"min\", \"max\"]].round(2))\n\nprint(\"\\nRegion shares (row-percent) by customer status\")\nprint(pd.crosstab(blue[\"region\"], blue[\"iscustomer\"], normalize=\"columns\")\n        .round(3).rename(columns={0: \"Non-cust\", 1: \"Cust\"}))\n\n# Simple Poisson likelihood & MLE\ndef pois_nll(lmbda, y):\n    l = lmbda[0]\n    return np.inf if l &lt;= 0 else -np.sum(y*np.log(l) - l - gammaln(y+1))\n\nλ_grid = np.linspace(0.1, Y_b.mean()*3, 400)\nll_vals = [-pois_nll([g], Y_b) for g in λ_grid]\n\nplt.figure()\nplt.plot(λ_grid, ll_vals)\nplt.axvline(Y_b.mean(), ls=\"--\", label=r\"$\\bar{Y}$\")\nplt.title(\"Log-likelihood for λ (Simple Poisson)\")\nplt.xlabel(\"λ\"); plt.ylabel(\"Log-likelihood\"); plt.legend()\nplt.tight_layout(); plt.savefig(\"figures/blue_ll_curve.png\", dpi=300)\n\noptλ = minimize(pois_nll, [Y_b.mean()], args=(Y_b,), bounds=[(1e-9, None)])\nprint(f\"\\nSimple-Poisson MLE λ̂ = {optλ.x[0]:.5g}  (sample mean = {Y_b.mean():.5g})\")\n\n# Poisson regression via statsmodels\nglm_blue = smf.glm(\"patents ~ age + age2 + C(region) + iscustomer\",\n                   data=blue, family=sm.families.Poisson()).fit()\nprint(\"\\nPoisson regression (Blueprinty) – tidy results\")\nprint(glm_blue.summary2().tables[1][[\"Coef.\",\"Std.Err.\",\"z\",\"P&gt;|z|\"]].round(4))\n\n# Average treatment effect\nmu_no  = glm_blue.predict(blue.assign(iscustomer=0))\nmu_yes = glm_blue.predict(blue.assign(iscustomer=1))\nprint(f\"\\nAvg predicted Δ patents if all became customers: {(mu_yes-mu_no).mean():.3f}\")\n\n# AIRBNB CASE\nair = (pd.read_csv(\"airbnb.csv\")\n         .assign(instant_bookable=lambda d:(d[\"instant_bookable\"]==\"t\").astype(int))\n         .dropna(subset=[\"number_of_reviews\",\"days\",\"room_type\",\"bathrooms\",\n                         \"bedrooms\",\"price\",\"review_scores_cleanliness\",\n                         \"review_scores_location\",\"review_scores_value\"]))\nprint(\"Rows after cleaning:\", len(air))\n\nplt.figure()\nair[\"number_of_reviews\"].hist(bins=50)\nplt.title(\"Review count distribution\")\nplt.xlabel(\"Number of reviews\"); plt.ylabel(\"Frequency\")\nplt.tight_layout(); plt.savefig(\"figures/air_reviews_hist.png\", dpi=300)\n\n# AirBnB Poisson regression (formula version avoids dtype issues)\nimport statsmodels.formula.api as smf\n\n# statsmodels will dummy-code room_type automatically with C(room_type)\nglm_air = smf.glm(\n    formula=\"\"\"number_of_reviews ~ days + bathrooms + bedrooms + price\n               + review_scores_cleanliness + review_scores_location\n               + review_scores_value + instant_bookable\n               + C(room_type)\"\"\",\n    data=air,\n    family=sm.families.Poisson()\n).fit()\n\nprint(\"\\nPoisson regression (Airbnb) – tidy results\")\nprint(glm_air.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"z\", \"P&gt;|z|\"]].round(4))\n\nprint(\"\\nFigures saved to ./figures — Done.\")"
  }
]