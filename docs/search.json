[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Machine Learning\n\n\n\n\n\n\nIdris Huang\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nIdris Huang\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nIdris Huang\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nIdris Huang\n\n\nApr 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/HW1/hw1_questions.html",
    "href": "blog/HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan and John List (2007) carried out a natural field experiment to investigate the effectiveness of different matching grant rates (1:1, 2:1, and 3:1) in charitable fundraising. In this work, I replicated and discussed some key results:\n\nChecking randomization balance\n\nEstimating the effect of any matching grant on donation probability\n\nComparing different match ratios\n\nExamining the size of donations\n\nDemonstrating the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) with simulations\n\n\n\nShow/Hide Code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\ndf_treat = df[df['treatment'] == 1]\ndf_cont  = df[df['control'] == 1]"
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#introduction",
    "href": "blog/HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan and John List (2007) carried out a natural field experiment to investigate the effectiveness of different matching grant rates (1:1, 2:1, and 3:1) in charitable fundraising. In this work, I replicated and discussed some key results:\n\nChecking randomization balance\n\nEstimating the effect of any matching grant on donation probability\n\nComparing different match ratios\n\nExamining the size of donations\n\nDemonstrating the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) with simulations\n\n\n\nShow/Hide Code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\ndf_treat = df[df['treatment'] == 1]\ndf_cont  = df[df['control'] == 1]"
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#data",
    "href": "blog/HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\n\nShow/Hide Code\nprint(\"Columns:\\n\", df.columns.tolist())\nprint(\"\\nSummary statistics:\\n\", df.describe())\n\n\nColumns:\n ['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25', 'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1', 'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra', 'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple', 'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse', 'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush', 'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack', 'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba', 'pop_propurban']\n\nSummary statistics:\n           treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168561      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378115     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258654  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nInterpretation: About 50,000 observations; key columns include treatment, control, gave, amount, etc."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#balance-test",
    "href": "blog/HW1/hw1_questions.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Test",
    "text": "Balance Test\n\n\nShow/Hide Code\nmrm2_treat = df_treat['mrm2'].dropna()\nmrm2_cont  = df_cont['mrm2'].dropna()\n\nt_stat, p_val, df_deg = ttest_ind(mrm2_treat, mrm2_cont, usevar='unequal')\nprint(f\"T-test for mrm2: t={t_stat:.4f}, p={p_val:.4g}, df={df_deg:.1f}\")\n\nmodel_bal = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nshort_summary(model_bal, label=\"OLS for mrm2 on treatment\")\n\n\nT-test for mrm2: t=0.1195, p=0.9049, df=33394.5\n=== OLS for mrm2 on treatment ===\n                coef   std err           t     P&gt;|t|\nIntercept  12.998142  0.093526  138.978873  0.000000\ntreatment   0.013686  0.114534    0.119492  0.904886\n\n\n\nResult: No significant difference in mrm2, suggesting balance."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#experimental-results",
    "href": "blog/HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\n1. Charitable Contribution Made\n\nA. Bar Plot\n\n\nShow/Hide Code\ngave_treat_mean = df_treat['gave'].mean()\ngave_cont_mean  = df_cont['gave'].mean()\nprint(\"Proportion gave - Control:\", gave_cont_mean, \"Treatment:\", gave_treat_mean)\n\n\nProportion gave - Control: 0.017858212980164198 Treatment: 0.02203856749311295\n\n\n\n\n\nBarplot_Proportion_Donors\n\n\n\n\nB. T-test & Regression\n\n\nShow/Hide Code\ngave_treat_vals = df_treat['gave']\ngave_cont_vals  = df_cont['gave']\nt_gave, p_gave, df_g = ttest_ind(gave_treat_vals, gave_cont_vals, usevar='unequal')\nprint(f\"gave T-test: t={t_gave:.4f}, p={p_gave:.4g}, df={df_g:.1f}\")\n\nmodel_gave_ols = smf.ols(\"gave ~ treatment\", data=df).fit()\nshort_summary(model_gave_ols, label=\"OLS for gave\")\n\nmodel_gave_probit = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nshort_summary(model_gave_probit, label=\"Probit for gave\")\n\n\ngave T-test: t=3.2095, p=0.001331, df=36576.8\n=== OLS for gave ===\n               coef   std err          t         P&gt;|t|\nIntercept  0.017858  0.001101  16.224643  4.779032e-59\ntreatment  0.004180  0.001348   3.101361  1.927403e-03\n\n=== Probit for gave ===\n               coef   std err         t     P&gt;|t|\nIntercept -2.100141  0.023316 -90.07277  0.000000\ntreatment  0.086785  0.027879   3.11293  0.001852\n\n\n\nResult: Matching grants significantly increase donation probability, though the effect size is small.\n\n\n\n2. Differences Between Match Rates\n\n\nShow/Hide Code\ngave_1to1 = df[df['ratio'] == 1]['gave']\ngave_2to1 = df[df['ratio2'] == 1]['gave']\ngave_3to1 = df[df['ratio3'] == 1]['gave']\n\nt_12, p_12, df_12 = ttest_ind(gave_1to1, gave_2to1, usevar='unequal')\nt_23, p_23, df_23 = ttest_ind(gave_2to1, gave_3to1, usevar='unequal')\nprint(f\"1:1 vs 2:1 =&gt; t={t_12:.4f}, p={p_12:.4g}\")\nprint(f\"2:1 vs 3:1 =&gt; t={t_23:.4f}, p={p_23:.4g}\")\n\nmodel_ratio = smf.ols(\"gave ~ ratio + ratio2 + ratio3\", data=df).fit()\nshort_summary(model_ratio, label=\"OLS for gave on ratio dummies\")\n\nr_1 = gave_1to1.mean()\nr_2 = gave_2to1.mean()\nr_3 = gave_3to1.mean()\nprint(\"Diff(1:1 vs 2:1):\", r_2 - r_1)\nprint(\"Diff(2:1 vs 3:1):\", r_3 - r_2)\n\n\n1:1 vs 2:1 =&gt; t=-0.9650, p=0.3345\n2:1 vs 3:1 =&gt; t=-0.0501, p=0.96\n=== OLS for gave on ratio dummies ===\n                    coef       std err          t         P&gt;|t|\nIntercept   1.802911e-02  1.386416e-03  13.004112  1.339110e-38\nratio[T.1]  2.732484e-03  1.914416e-03   1.427320  1.534940e-01\nratio[T.2] -3.535509e+10  2.437258e+11  -0.145061  8.846633e-01\nratio[T.3] -1.520186e+09  1.038114e+10  -0.146437  8.835767e-01\nratio2      3.535509e+10  2.437258e+11   0.145061  8.846633e-01\nratio3      1.520186e+09  1.038114e+10   0.146437  8.835767e-01\n\nDiff(1:1 vs 2:1): 0.0018842510217149944\nDiff(2:1 vs 3:1): 0.00010002398025293902\n\n\nResult: No evidence that higher match ratios lead to significantly greater giving.\n\n\n3. Size of Charitable Contribution\n\nA. Unconditional and Conditional\n\n\nShow/Hide Code\namt_treat = df_treat['amount'].fillna(0)\namt_cont  = df_cont['amount'].fillna(0)\nt_amt, p_amt, _ = ttest_ind(amt_treat, amt_cont, usevar='unequal')\nprint(f\"Uncond. amount T-test: t={t_amt:.4f}, p={p_amt:.4g}\")\n\nmodel_amt_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nshort_summary(model_amt_ols, label=\"OLS for unconditional 'amount'\")\n\ndf_givers = df[df['gave'] == 1]\namt_treat_g = df_givers[df_givers['treatment']==1]['amount']\namt_cont_g  = df_givers[df_givers['control'] ==1]['amount']\n\nt_amt_g, p_amt_g, _ = ttest_ind(amt_treat_g, amt_cont_g, usevar='unequal')\nprint(f\"Cond. amount T-test: t={t_amt_g:.4f}, p={p_amt_g:.4g}\")\n\nmodel_amt_cond_ols = smf.ols(\"amount ~ treatment\", data=df_givers).fit()\nshort_summary(model_amt_cond_ols, label=\"OLS for amount among donors\")\n\n\nUncond. amount T-test: t=1.9183, p=0.05509\n=== OLS for unconditional 'amount' ===\n               coef   std err          t         P&gt;|t|\nIntercept  0.813268  0.067418  12.062995  1.843438e-33\ntreatment  0.153605  0.082561   1.860503  6.282029e-02\n\nCond. amount T-test: t=-0.5846, p=0.559\n=== OLS for amount among donors ===\n                coef   std err          t         P&gt;|t|\nIntercept  45.540268  2.423378  18.792063  5.473578e-68\ntreatment  -1.668393  2.872384  -0.580839  5.614756e-01\n\n\n\nResult: Very small/unreliable difference in donation amounts.\n\n\nB. Histograms\n\n\nInterpretation: Distributions appear similar between groups."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#simulation-experiments",
    "href": "blog/HW1/hw1_questions.html#simulation-experiments",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiments",
    "text": "Simulation Experiments\n\nLaw of Large Numbers\n\n\nShow/Hide Code\nN_sims = 10000\np_c = 0.018\np_t = 0.022\n\nsim_c = np.random.binomial(1, p_c, N_sims)\nsim_t = np.random.binomial(1, p_t, N_sims)\ndiff_vec = sim_t - sim_c\ncum_avg = np.cumsum(diff_vec)/np.arange(1,N_sims+1)\n\n\n\n\n\nCumulative Average difference plot\n\n\nInterpretation: Cumulative average converges near 0.004 (the true difference in proportions).\n\n\nCentral Limit Theorem\n\n\nShow/Hide Code\ndef draw_mean_diffs(n_control, n_treatment, iters=1000):\n    out = []\n    for _ in range(iters):\n        c_draws = np.random.binomial(1, p_c, n_control)\n        t_draws = np.random.binomial(1, p_t, n_treatment)\n        out.append(t_draws.mean() - c_draws.mean())\n    return np.array(out)\n\nfor s in [50, 200, 500, 1000]:\n    diffs = draw_mean_diffs(s, s)\n\n\n\n\n\n\nInterpretation: As n increases, the sampling distribution becomes tighter and more bell-shaped."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#conclusion",
    "href": "blog/HW1/hw1_questions.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThe results replicate those of Karlan and List (2007): matching grants increase donation rates, but higher match ratios do not improve outcomes further. The average donation amount is unaffected. Simulations illustrate the LLN and CLT in practice."
  },
  {
    "objectID": "blog/HW1/hw1_questions.html#full-code",
    "href": "blog/HW1/hw1_questions.html#full-code",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Full Code",
    "text": "Full Code\n\n\nShow/Hide Code\n# A Replication of Karlan and List (2007)\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\n\n# Balance Test\ndf_treat = df[df['treatment'] == 1]\ndf_cont = df[df['control'] == 1]\n\nmrm2_treat = df_treat['mrm2'].dropna()\nmrm2_cont = df_cont['mrm2'].dropna()\nt_stat, p_val, df_deg = ttest_ind(mrm2_treat, mrm2_cont, usevar='unequal')\nmodel_bal = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nshort_summary(model_bal, label=\"OLS for mrm2 on treatment\")\n\n# Charitable Contributions\ngave_treat_mean = df_treat['gave'].mean()\ngave_cont_mean = df_cont['gave'].mean()\nplt.bar([\"Control\", \"Treatment\"], [gave_cont_mean, gave_treat_mean], color=[\"#1f77b4\",\"#ff7f0e\"])\nplt.title(\"Proportion Who Donated: Control vs Treatment\")\nplt.ylabel(\"Proportion\")\nplt.show()\n\ngave_treat_vals = df_treat['gave']\ngave_cont_vals = df_cont['gave']\nt_gave, p_gave, df_g = ttest_ind(gave_treat_vals, gave_cont_vals, usevar='unequal')\nmodel_gave_ols = smf.ols(\"gave ~ treatment\", data=df).fit()\nshort_summary(model_gave_ols, label=\"OLS for gave on treatment\")\nmodel_gave_probit = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nshort_summary(model_gave_probit, label=\"Probit for gave on treatment\")\n\n# Match Ratios\ngave_1to1 = df[df['ratio'] == 1]['gave']\ngave_2to1 = df[df['ratio2'] == 1]['gave']\ngave_3to1 = df[df['ratio3'] == 1]['gave']\nt_12, p_12, df_12 = ttest_ind(gave_1to1, gave_2to1, usevar='unequal')\nt_23, p_23, df_23 = ttest_ind(gave_2to1, gave_3to1, usevar='unequal')\nmodel_ratio = smf.ols(\"gave ~ ratio + ratio2 + ratio3\", data=df).fit()\nshort_summary(model_ratio, label=\"OLS for gave on ratio dummies\")\nprint(\"Response rate difference, 1:1 vs 2:1 =\", (gave_2to1.mean() - gave_1to1.mean())*100)\nprint(\"Response rate difference, 2:1 vs 3:1 =\", (gave_3to1.mean() - gave_2to1.mean())*100)\n\n# Donation Amounts\namt_treat = df_treat['amount'].fillna(0)\namt_cont = df_cont['amount'].fillna(0)\nt_amt, p_amt, df_amt_ = ttest_ind(amt_treat, amt_cont, usevar='unequal')\nmodel_amt_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nshort_summary(model_amt_ols, label=\"OLS for unconditional 'amount'\")\n\ndf_givers = df[df['gave'] == 1]\namt_treat_g = df_givers[df_givers['treatment']==1]['amount']\namt_cont_g = df_givers[df_givers['control']==1]['amount']\nt_amt_g, p_amt_g, df_amt_g = ttest_ind(amt_treat_g, amt_cont_g, usevar='unequal')\nmodel_amt_cond_ols = smf.ols(\"amount ~ treatment\", data=df_givers).fit()\nshort_summary(model_amt_cond_ols, label=\"OLS for amount among donors\")\n\n# Histograms\nmean_treat_g = amt_treat_g.mean()\nmean_cont_g = amt_cont_g.mean()\n\nplt.hist(amt_treat_g, bins=30, color=\"#9467bd\")\nplt.axvline(mean_treat_g, color='red', linestyle='--', linewidth=2)\nplt.title(\"Histogram: Donation Amounts (Treatment donors)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.hist(amt_cont_g, bins=30, color=\"#d62728\")\nplt.axvline(mean_cont_g, color='red', linestyle='--', linewidth=2)\nplt.title(\"Histogram: Donation Amounts (Control donors)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# LLN Simulation\nN_sims = 10000\np_c = 0.018\np_t = 0.022\nsim_c = np.random.binomial(1, p_c, size=N_sims)\nsim_t = np.random.binomial(1, p_t, size=N_sims)\ndiff_vec = sim_t - sim_c\ncum_avg = np.cumsum(diff_vec) / np.arange(1, N_sims+1)\n\nplt.plot(cum_avg, color=\"#2ca02c\")\nplt.axhline(y=(p_t - p_c), color='red', linestyle='--')\nplt.title(\"Cumulative Average of Differences (Treatment - Control)\")\nplt.xlabel(\"Number of draws\")\nplt.ylabel(\"Cumulative average difference\")\nplt.show()\n\n# CLT Simulation\ndef draw_mean_diffs(n_control, n_treatment, iters=1000):\n    out = []\n    for _ in range(iters):\n        c_draws = np.random.binomial(1, p_c, size=n_control)\n        t_draws = np.random.binomial(1, p_t, size=n_treatment)\n        out.append(t_draws.mean() - c_draws.mean())\n    return np.array(out)\n\nfor s in [50, 200, 500, 1000]:\n    diffs_s = draw_mean_diffs(s, s)\n    plt.hist(diffs_s, bins=30, color=\"#ff7f0e\", alpha=0.8)\n    plt.axvline(0, color='black', linestyle='--')\n    plt.title(f\"Histogram of Differences (n={s}, 1000 reps)\")\n    plt.xlabel(\"Difference in means (treat - control)\")\n    plt.ylabel(\"Frequency\")\n    plt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Idris Huang",
    "section": "",
    "text": "Here is a paragraph about me!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project 2/index.html",
    "href": "blog/project 2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data"
  },
  {
    "objectID": "blog/project 2/index.html#section-1-data",
    "href": "blog/project 2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project 2/index.html#section-2-analysis",
    "href": "blog/project 2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/HW1/index.html",
    "href": "blog/HW1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan and John List (2007) carried out a natural field experiment to investigate the effectiveness of different matching grant rates (1:1, 2:1, and 3:1) in charitable fundraising. In this work, I replicated and discussed some key results:\n\nChecking randomization balance\n\nEstimating the effect of any matching grant on donation probability\n\nComparing different match ratios\n\nExamining the size of donations\n\nDemonstrating the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) with simulations\n\n\n\nShow/Hide Code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\ndf_treat = df[df['treatment'] == 1]\ndf_cont  = df[df['control'] == 1]"
  },
  {
    "objectID": "blog/HW1/index.html#introduction",
    "href": "blog/HW1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan and John List (2007) carried out a natural field experiment to investigate the effectiveness of different matching grant rates (1:1, 2:1, and 3:1) in charitable fundraising. In this work, I replicated and discussed some key results:\n\nChecking randomization balance\n\nEstimating the effect of any matching grant on donation probability\n\nComparing different match ratios\n\nExamining the size of donations\n\nDemonstrating the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) with simulations\n\n\n\nShow/Hide Code\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\ndf_treat = df[df['treatment'] == 1]\ndf_cont  = df[df['control'] == 1]"
  },
  {
    "objectID": "blog/HW1/index.html#data",
    "href": "blog/HW1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\n\nShow/Hide Code\nprint(\"Columns:\\n\", df.columns.tolist())\nprint(\"\\nSummary statistics:\\n\", df.describe())\n\n\nColumns:\n ['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25', 'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1', 'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra', 'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple', 'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse', 'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush', 'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack', 'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba', 'pop_propurban']\n\nSummary statistics:\n           treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168561      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378115     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258654  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nInterpretation: About 50,000 observations; key columns include treatment, control, gave, amount, etc."
  },
  {
    "objectID": "blog/HW1/index.html#balance-test",
    "href": "blog/HW1/index.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Test",
    "text": "Balance Test\n\n\nShow/Hide Code\nmrm2_treat = df_treat['mrm2'].dropna()\nmrm2_cont  = df_cont['mrm2'].dropna()\n\nt_stat, p_val, df_deg = ttest_ind(mrm2_treat, mrm2_cont, usevar='unequal')\nprint(f\"T-test for mrm2: t={t_stat:.4f}, p={p_val:.4g}, df={df_deg:.1f}\")\n\nmodel_bal = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nshort_summary(model_bal, label=\"OLS for mrm2 on treatment\")\n\n\nT-test for mrm2: t=0.1195, p=0.9049, df=33394.5\n=== OLS for mrm2 on treatment ===\n                coef   std err           t     P&gt;|t|\nIntercept  12.998142  0.093526  138.978873  0.000000\ntreatment   0.013686  0.114534    0.119492  0.904886\n\n\n\nResult: No significant difference in mrm2, suggesting balance."
  },
  {
    "objectID": "blog/HW1/index.html#experimental-results",
    "href": "blog/HW1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\n1. Charitable Contribution Made\n\nA. Bar Plot\n\n\nShow/Hide Code\ngave_treat_mean = df_treat['gave'].mean()\ngave_cont_mean  = df_cont['gave'].mean()\nprint(\"Proportion gave - Control:\", gave_cont_mean, \"Treatment:\", gave_treat_mean)\n\n\nProportion gave - Control: 0.017858212980164198 Treatment: 0.02203856749311295\n\n\n\n\n\nBarplot_Proportion_Donors\n\n\n\n\nB. T-test & Regression\n\n\nShow/Hide Code\ngave_treat_vals = df_treat['gave']\ngave_cont_vals  = df_cont['gave']\nt_gave, p_gave, df_g = ttest_ind(gave_treat_vals, gave_cont_vals, usevar='unequal')\nprint(f\"gave T-test: t={t_gave:.4f}, p={p_gave:.4g}, df={df_g:.1f}\")\n\nmodel_gave_ols = smf.ols(\"gave ~ treatment\", data=df).fit()\nshort_summary(model_gave_ols, label=\"OLS for gave\")\n\nmodel_gave_probit = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nshort_summary(model_gave_probit, label=\"Probit for gave\")\n\n\ngave T-test: t=3.2095, p=0.001331, df=36576.8\n=== OLS for gave ===\n               coef   std err          t         P&gt;|t|\nIntercept  0.017858  0.001101  16.224643  4.779032e-59\ntreatment  0.004180  0.001348   3.101361  1.927403e-03\n\n=== Probit for gave ===\n               coef   std err         t     P&gt;|t|\nIntercept -2.100141  0.023316 -90.07277  0.000000\ntreatment  0.086785  0.027879   3.11293  0.001852\n\n\n\nResult: Matching grants significantly increase donation probability, though the effect size is small.\n\n\n\n2. Differences Between Match Rates\n\n\nShow/Hide Code\ngave_1to1 = df[df['ratio'] == 1]['gave']\ngave_2to1 = df[df['ratio2'] == 1]['gave']\ngave_3to1 = df[df['ratio3'] == 1]['gave']\n\nt_12, p_12, df_12 = ttest_ind(gave_1to1, gave_2to1, usevar='unequal')\nt_23, p_23, df_23 = ttest_ind(gave_2to1, gave_3to1, usevar='unequal')\nprint(f\"1:1 vs 2:1 =&gt; t={t_12:.4f}, p={p_12:.4g}\")\nprint(f\"2:1 vs 3:1 =&gt; t={t_23:.4f}, p={p_23:.4g}\")\n\nmodel_ratio = smf.ols(\"gave ~ ratio + ratio2 + ratio3\", data=df).fit()\nshort_summary(model_ratio, label=\"OLS for gave on ratio dummies\")\n\nr_1 = gave_1to1.mean()\nr_2 = gave_2to1.mean()\nr_3 = gave_3to1.mean()\nprint(\"Diff(1:1 vs 2:1):\", r_2 - r_1)\nprint(\"Diff(2:1 vs 3:1):\", r_3 - r_2)\n\n\n1:1 vs 2:1 =&gt; t=-0.9650, p=0.3345\n2:1 vs 3:1 =&gt; t=-0.0501, p=0.96\n=== OLS for gave on ratio dummies ===\n                    coef       std err          t         P&gt;|t|\nIntercept   1.802911e-02  1.386416e-03  13.004112  1.339110e-38\nratio[T.1]  2.732484e-03  1.914416e-03   1.427320  1.534940e-01\nratio[T.2] -3.535509e+10  2.437258e+11  -0.145061  8.846633e-01\nratio[T.3] -1.520186e+09  1.038114e+10  -0.146437  8.835767e-01\nratio2      3.535509e+10  2.437258e+11   0.145061  8.846633e-01\nratio3      1.520186e+09  1.038114e+10   0.146437  8.835767e-01\n\nDiff(1:1 vs 2:1): 0.0018842510217149944\nDiff(2:1 vs 3:1): 0.00010002398025293902\n\n\nResult: No evidence that higher match ratios lead to significantly greater giving.\n\n\n3. Size of Charitable Contribution\n\nA. Unconditional and Conditional\n\n\nShow/Hide Code\namt_treat = df_treat['amount'].fillna(0)\namt_cont  = df_cont['amount'].fillna(0)\nt_amt, p_amt, _ = ttest_ind(amt_treat, amt_cont, usevar='unequal')\nprint(f\"Uncond. amount T-test: t={t_amt:.4f}, p={p_amt:.4g}\")\n\nmodel_amt_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nshort_summary(model_amt_ols, label=\"OLS for unconditional 'amount'\")\n\ndf_givers = df[df['gave'] == 1]\namt_treat_g = df_givers[df_givers['treatment']==1]['amount']\namt_cont_g  = df_givers[df_givers['control'] ==1]['amount']\n\nt_amt_g, p_amt_g, _ = ttest_ind(amt_treat_g, amt_cont_g, usevar='unequal')\nprint(f\"Cond. amount T-test: t={t_amt_g:.4f}, p={p_amt_g:.4g}\")\n\nmodel_amt_cond_ols = smf.ols(\"amount ~ treatment\", data=df_givers).fit()\nshort_summary(model_amt_cond_ols, label=\"OLS for amount among donors\")\n\n\nUncond. amount T-test: t=1.9183, p=0.05509\n=== OLS for unconditional 'amount' ===\n               coef   std err          t         P&gt;|t|\nIntercept  0.813268  0.067418  12.062995  1.843438e-33\ntreatment  0.153605  0.082561   1.860503  6.282029e-02\n\nCond. amount T-test: t=-0.5846, p=0.559\n=== OLS for amount among donors ===\n                coef   std err          t         P&gt;|t|\nIntercept  45.540268  2.423378  18.792063  5.473578e-68\ntreatment  -1.668393  2.872384  -0.580839  5.614756e-01\n\n\n\nResult: Very small/unreliable difference in donation amounts.\n\n\nB. Histograms\n\n\nInterpretation: Distributions appear similar between groups."
  },
  {
    "objectID": "blog/HW1/index.html#simulation-experiments",
    "href": "blog/HW1/index.html#simulation-experiments",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiments",
    "text": "Simulation Experiments\n\nLaw of Large Numbers\n\n\nShow/Hide Code\nN_sims = 10000\np_c = 0.018\np_t = 0.022\n\nsim_c = np.random.binomial(1, p_c, N_sims)\nsim_t = np.random.binomial(1, p_t, N_sims)\ndiff_vec = sim_t - sim_c\ncum_avg = np.cumsum(diff_vec)/np.arange(1,N_sims+1)\n\n\n\n\n\nCumulative Average difference plot\n\n\nInterpretation: Cumulative average converges near 0.004 (the true difference in proportions).\n\n\nCentral Limit Theorem\n\n\nShow/Hide Code\ndef draw_mean_diffs(n_control, n_treatment, iters=1000):\n    out = []\n    for _ in range(iters):\n        c_draws = np.random.binomial(1, p_c, n_control)\n        t_draws = np.random.binomial(1, p_t, n_treatment)\n        out.append(t_draws.mean() - c_draws.mean())\n    return np.array(out)\n\nfor s in [50, 200, 500, 1000]:\n    diffs = draw_mean_diffs(s, s)\n\n\n\n\n\n\nInterpretation: As n increases, the sampling distribution becomes tighter and more bell-shaped."
  },
  {
    "objectID": "blog/HW1/index.html#conclusion",
    "href": "blog/HW1/index.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThe results replicate those of Karlan and List (2007): matching grants increase donation rates, but higher match ratios do not improve outcomes further. The average donation amount is unaffected. Simulations illustrate the LLN and CLT in practice."
  },
  {
    "objectID": "blog/HW1/index.html#full-code",
    "href": "blog/HW1/index.html#full-code",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Full Code",
    "text": "Full Code\n\n\nShow/Hide Code\n# A Replication of Karlan and List (2007)\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.weightstats import ttest_ind\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=[\"#2ca02c\",\"#ff7f0e\",\"#9467bd\",\"#d62728\"])\nplt.rcParams['figure.figsize'] = (7,5)\n\ndef short_summary(model, label=None):\n    if label:\n        print(f\"=== {label} ===\")\n    df_summary = pd.DataFrame({\n        'coef': model.params,\n        'std err': model.bse,\n        't': model.tvalues,\n        'P&gt;|t|': model.pvalues\n    })\n    print(df_summary)\n    print()\n\ndata_filepath = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_filepath)\n\n# Balance Test\ndf_treat = df[df['treatment'] == 1]\ndf_cont = df[df['control'] == 1]\n\nmrm2_treat = df_treat['mrm2'].dropna()\nmrm2_cont = df_cont['mrm2'].dropna()\nt_stat, p_val, df_deg = ttest_ind(mrm2_treat, mrm2_cont, usevar='unequal')\nmodel_bal = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nshort_summary(model_bal, label=\"OLS for mrm2 on treatment\")\n\n# Charitable Contributions\ngave_treat_mean = df_treat['gave'].mean()\ngave_cont_mean = df_cont['gave'].mean()\nplt.bar([\"Control\", \"Treatment\"], [gave_cont_mean, gave_treat_mean], color=[\"#1f77b4\",\"#ff7f0e\"])\nplt.title(\"Proportion Who Donated: Control vs Treatment\")\nplt.ylabel(\"Proportion\")\nplt.show()\n\ngave_treat_vals = df_treat['gave']\ngave_cont_vals = df_cont['gave']\nt_gave, p_gave, df_g = ttest_ind(gave_treat_vals, gave_cont_vals, usevar='unequal')\nmodel_gave_ols = smf.ols(\"gave ~ treatment\", data=df).fit()\nshort_summary(model_gave_ols, label=\"OLS for gave on treatment\")\nmodel_gave_probit = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nshort_summary(model_gave_probit, label=\"Probit for gave on treatment\")\n\n# Match Ratios\ngave_1to1 = df[df['ratio'] == 1]['gave']\ngave_2to1 = df[df['ratio2'] == 1]['gave']\ngave_3to1 = df[df['ratio3'] == 1]['gave']\nt_12, p_12, df_12 = ttest_ind(gave_1to1, gave_2to1, usevar='unequal')\nt_23, p_23, df_23 = ttest_ind(gave_2to1, gave_3to1, usevar='unequal')\nmodel_ratio = smf.ols(\"gave ~ ratio + ratio2 + ratio3\", data=df).fit()\nshort_summary(model_ratio, label=\"OLS for gave on ratio dummies\")\nprint(\"Response rate difference, 1:1 vs 2:1 =\", (gave_2to1.mean() - gave_1to1.mean())*100)\nprint(\"Response rate difference, 2:1 vs 3:1 =\", (gave_3to1.mean() - gave_2to1.mean())*100)\n\n# Donation Amounts\namt_treat = df_treat['amount'].fillna(0)\namt_cont = df_cont['amount'].fillna(0)\nt_amt, p_amt, df_amt_ = ttest_ind(amt_treat, amt_cont, usevar='unequal')\nmodel_amt_ols = smf.ols(\"amount ~ treatment\", data=df).fit()\nshort_summary(model_amt_ols, label=\"OLS for unconditional 'amount'\")\n\ndf_givers = df[df['gave'] == 1]\namt_treat_g = df_givers[df_givers['treatment']==1]['amount']\namt_cont_g = df_givers[df_givers['control']==1]['amount']\nt_amt_g, p_amt_g, df_amt_g = ttest_ind(amt_treat_g, amt_cont_g, usevar='unequal')\nmodel_amt_cond_ols = smf.ols(\"amount ~ treatment\", data=df_givers).fit()\nshort_summary(model_amt_cond_ols, label=\"OLS for amount among donors\")\n\n# Histograms\nmean_treat_g = amt_treat_g.mean()\nmean_cont_g = amt_cont_g.mean()\n\nplt.hist(amt_treat_g, bins=30, color=\"#9467bd\")\nplt.axvline(mean_treat_g, color='red', linestyle='--', linewidth=2)\nplt.title(\"Histogram: Donation Amounts (Treatment donors)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.hist(amt_cont_g, bins=30, color=\"#d62728\")\nplt.axvline(mean_cont_g, color='red', linestyle='--', linewidth=2)\nplt.title(\"Histogram: Donation Amounts (Control donors)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# LLN Simulation\nN_sims = 10000\np_c = 0.018\np_t = 0.022\nsim_c = np.random.binomial(1, p_c, size=N_sims)\nsim_t = np.random.binomial(1, p_t, size=N_sims)\ndiff_vec = sim_t - sim_c\ncum_avg = np.cumsum(diff_vec) / np.arange(1, N_sims+1)\n\nplt.plot(cum_avg, color=\"#2ca02c\")\nplt.axhline(y=(p_t - p_c), color='red', linestyle='--')\nplt.title(\"Cumulative Average of Differences (Treatment - Control)\")\nplt.xlabel(\"Number of draws\")\nplt.ylabel(\"Cumulative average difference\")\nplt.show()\n\n# CLT Simulation\ndef draw_mean_diffs(n_control, n_treatment, iters=1000):\n    out = []\n    for _ in range(iters):\n        c_draws = np.random.binomial(1, p_c, size=n_control)\n        t_draws = np.random.binomial(1, p_t, size=n_treatment)\n        out.append(t_draws.mean() - c_draws.mean())\n    return np.array(out)\n\nfor s in [50, 200, 500, 1000]:\n    diffs_s = draw_mean_diffs(s, s)\n    plt.hist(diffs_s, bins=30, color=\"#ff7f0e\", alpha=0.8)\n    plt.axvline(0, color='black', linestyle='--')\n    plt.title(f\"Histogram of Differences (n={s}, 1000 reps)\")\n    plt.xlabel(\"Difference in means (treat - control)\")\n    plt.ylabel(\"Frequency\")\n    plt.show()"
  },
  {
    "objectID": "blog/HW2/index.html",
    "href": "blog/HW2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last five years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nShow/Hide Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\n\n\n\n\n\n\nShow/Hide Code\nblue = (pd.read_csv(\"blueprinty.csv\")\n          .replace([np.inf, -np.inf], np.nan)\n          .dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"]))\nblue[\"age2\"] = blue[\"age\"]**2\n\nprint(\"--- Patents by customer status ---\")\nprint(blue.groupby(\"iscustomer\")[\"patents\"].describe()[[\"mean\",\"std\"]])\nprint(\"\\n--- Age by customer status ---\")\nprint(blue.groupby(\"iscustomer\")[\"age\"].describe()[[\"mean\",\"std\",\"min\",\"max\"]])\nprint(\"\\n--- Region shares ---\")\nprint(pd.crosstab(blue[\"region\"], blue[\"iscustomer\"], normalize=\"columns\").round(3))\n\n\n--- Patents by customer status ---\n                mean       std\niscustomer                    \n0           3.473013  2.225060\n1           4.133056  2.546846\n\n--- Age by customer status ---\n                 mean       std   min   max\niscustomer                                 \n0           26.101570  6.945426   9.0  47.5\n1           26.900208  7.814678  10.0  49.0\n\n--- Region shares ---\niscustomer      0      1\nregion                  \nMidwest     0.184  0.077\nNortheast   0.268  0.682\nNorthwest   0.155  0.060\nSouth       0.153  0.073\nSouthwest   0.240  0.108\n\n\n\n\nShow/Hide Code\nfig, ax = plt.subplots()\nfor cust, grp in blue.groupby(\"iscustomer\"):\n    ax.hist(grp[\"patents\"],\n            bins=np.arange(grp[\"patents\"].max()+2)-0.5,\n            alpha=0.6, label=f\"Customer = {cust}\")\nax.set(title=\"Patent Count by Customer Status\",\n       xlabel=\"Patents (last 5 yrs)\", ylabel=\"Frequency\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last five years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nShow/Hide Code\nY = blue[\"patents\"].astype(int).to_numpy()\n\ndef nll(lmbda):\n    l = lmbda[0]\n    return np.inf if l&lt;=0 else -(Y*np.log(l)-l-gammaln(Y+1)).sum()\n\nλ_grid = np.linspace(0.1, Y.mean()*3, 400)\nll_vals = [-nll([g]) for g in λ_grid]\n\nplt.plot(λ_grid, ll_vals)\nplt.axvline(Y.mean(), ls=\"--\", label=r\"$\\bar{Y}$\")\nplt.title(\"Log-likelihood for λ (Simple Poisson)\")\nplt.xlabel(\"λ\"); plt.ylabel(\"Log-likelihood\"); plt.legend()\nplt.show()\n\nmle = minimize(nll, [Y.mean()], bounds=[(1e-9,None)])\nprint(\"MLE λ̂ =\", round(mle.x[0], 4))\n\n\n\n\n\n\n\n\n\nMLE λ̂ = 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i=\\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow/Hide Code\nglm_blue = smf.glm(\"patents ~ age + age2 + C(region) + iscustomer\",\n                   data=blue, family=sm.families.Poisson()).fit()\nprint(glm_blue.summary2().tables[1][[\"Coef.\",\"Std.Err.\",\"z\",\"P&gt;|z|\"]].round(4))\n\n\n                         Coef.  Std.Err.        z   P&gt;|z|\nIntercept              -0.5089    0.1832  -2.7783  0.0055\nC(region)[T.Northeast]  0.0292    0.0436   0.6686  0.5037\nC(region)[T.Northwest] -0.0176    0.0538  -0.3268  0.7438\nC(region)[T.South]      0.0566    0.0527   1.0740  0.2828\nC(region)[T.Southwest]  0.0506    0.0472   1.0716  0.2839\nage                     0.1486    0.0139  10.7162  0.0000\nage2                   -0.0030    0.0003 -11.5132  0.0000\niscustomer              0.2076    0.0309   6.7192  0.0000\n\n\n\n\nShow/Hide Code\nmu_no  = glm_blue.predict(blue.assign(iscustomer=0))\nmu_yes = glm_blue.predict(blue.assign(iscustomer=1))\nprint(\"Average predicted Δ patents if all became customers:\",\n      round((mu_yes-mu_no).mean(),3))\n\n\nAverage predicted Δ patents if all became customers: 0.793\n\n\nInterpretation: The customer coefficient β̂ ≈ 0.208 (p &lt; 0.001) implies Blueprinty users file ≈ 23 % more patents; the average predicted gain is 0.79 patents over five years."
  },
  {
    "objectID": "blog/HW2/index.html#section-1-data",
    "href": "blog/HW2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/HW2/index.html#section-2-analysis",
    "href": "blog/HW2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "I analyzed the data."
  },
  {
    "objectID": "blog/HW2/index.html#blueprinty-case-study",
    "href": "blog/HW2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last five years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nShow/Hide Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\n\n\n\n\n\n\nShow/Hide Code\nblue = (pd.read_csv(\"blueprinty.csv\")\n          .replace([np.inf, -np.inf], np.nan)\n          .dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"]))\nblue[\"age2\"] = blue[\"age\"]**2\n\nprint(\"--- Patents by customer status ---\")\nprint(blue.groupby(\"iscustomer\")[\"patents\"].describe()[[\"mean\",\"std\"]])\nprint(\"\\n--- Age by customer status ---\")\nprint(blue.groupby(\"iscustomer\")[\"age\"].describe()[[\"mean\",\"std\",\"min\",\"max\"]])\nprint(\"\\n--- Region shares ---\")\nprint(pd.crosstab(blue[\"region\"], blue[\"iscustomer\"], normalize=\"columns\").round(3))\n\n\n--- Patents by customer status ---\n                mean       std\niscustomer                    \n0           3.473013  2.225060\n1           4.133056  2.546846\n\n--- Age by customer status ---\n                 mean       std   min   max\niscustomer                                 \n0           26.101570  6.945426   9.0  47.5\n1           26.900208  7.814678  10.0  49.0\n\n--- Region shares ---\niscustomer      0      1\nregion                  \nMidwest     0.184  0.077\nNortheast   0.268  0.682\nNorthwest   0.155  0.060\nSouth       0.153  0.073\nSouthwest   0.240  0.108\n\n\n\n\nShow/Hide Code\nfig, ax = plt.subplots()\nfor cust, grp in blue.groupby(\"iscustomer\"):\n    ax.hist(grp[\"patents\"],\n            bins=np.arange(grp[\"patents\"].max()+2)-0.5,\n            alpha=0.6, label=f\"Customer = {cust}\")\nax.set(title=\"Patent Count by Customer Status\",\n       xlabel=\"Patents (last 5 yrs)\", ylabel=\"Frequency\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last five years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nShow/Hide Code\nY = blue[\"patents\"].astype(int).to_numpy()\n\ndef nll(lmbda):\n    l = lmbda[0]\n    return np.inf if l&lt;=0 else -(Y*np.log(l)-l-gammaln(Y+1)).sum()\n\nλ_grid = np.linspace(0.1, Y.mean()*3, 400)\nll_vals = [-nll([g]) for g in λ_grid]\n\nplt.plot(λ_grid, ll_vals)\nplt.axvline(Y.mean(), ls=\"--\", label=r\"$\\bar{Y}$\")\nplt.title(\"Log-likelihood for λ (Simple Poisson)\")\nplt.xlabel(\"λ\"); plt.ylabel(\"Log-likelihood\"); plt.legend()\nplt.show()\n\nmle = minimize(nll, [Y.mean()], bounds=[(1e-9,None)])\nprint(\"MLE λ̂ =\", round(mle.x[0], 4))\n\n\n\n\n\n\n\n\n\nMLE λ̂ = 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i=\\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow/Hide Code\nglm_blue = smf.glm(\"patents ~ age + age2 + C(region) + iscustomer\",\n                   data=blue, family=sm.families.Poisson()).fit()\nprint(glm_blue.summary2().tables[1][[\"Coef.\",\"Std.Err.\",\"z\",\"P&gt;|z|\"]].round(4))\n\n\n                         Coef.  Std.Err.        z   P&gt;|z|\nIntercept              -0.5089    0.1832  -2.7783  0.0055\nC(region)[T.Northeast]  0.0292    0.0436   0.6686  0.5037\nC(region)[T.Northwest] -0.0176    0.0538  -0.3268  0.7438\nC(region)[T.South]      0.0566    0.0527   1.0740  0.2828\nC(region)[T.Southwest]  0.0506    0.0472   1.0716  0.2839\nage                     0.1486    0.0139  10.7162  0.0000\nage2                   -0.0030    0.0003 -11.5132  0.0000\niscustomer              0.2076    0.0309   6.7192  0.0000\n\n\n\n\nShow/Hide Code\nmu_no  = glm_blue.predict(blue.assign(iscustomer=0))\nmu_yes = glm_blue.predict(blue.assign(iscustomer=1))\nprint(\"Average predicted Δ patents if all became customers:\",\n      round((mu_yes-mu_no).mean(),3))\n\n\nAverage predicted Δ patents if all became customers: 0.793\n\n\nInterpretation: The customer coefficient β̂ ≈ 0.208 (p &lt; 0.001) implies Blueprinty users file ≈ 23 % more patents; the average predicted gain is 0.79 patents over five years."
  },
  {
    "objectID": "blog/HW2/index.html#airbnb-case-study",
    "href": "blog/HW2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped 40 000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\nid – unique ID number for each unit\n\nlast_scraped, host_since, days – listing age variables\n\nroom_type – Entire home/apt, Private room, Shared room\n\nbathrooms, bedrooms – amenities\n\nprice – nightly price (USD)\n\nnumber_of_reviews – proxy for bookings\n\nreview_scores_cleanliness, review_scores_location, review_scores_value – 1-10 ratings\n\ninstant_bookable – \"t\" if instantly bookable\n\n\n\n\n\n\nData\n\n\nShow/Hide Code\nair = (pd.read_csv(\"airbnb.csv\")\n         .assign(instant_bookable=lambda d:(d[\"instant_bookable\"]==\"t\").astype(int))\n         .dropna(subset=[\"number_of_reviews\",\"days\",\"room_type\",\"bathrooms\",\n                         \"bedrooms\",\"price\",\n                         \"review_scores_cleanliness\",\"review_scores_location\",\n                         \"review_scores_value\"]))\nprint(\"Rows after cleaning:\", len(air))\n\n\nRows after cleaning: 30160\n\n\n\n\nShow/Hide Code\nair[\"number_of_reviews\"].hist(bins=50)\nplt.title(\"Review Count Distribution\")\nplt.xlabel(\"Number of Reviews\"); plt.ylabel(\"Frequency\")\nplt.tight_layout(); plt.show()\n\n\n\n\n\n\n\n\n\n\n\nEstimation of Poisson Regression Model\n\n\nShow/Hide Code\nglm_air = smf.glm(\n    formula=\"\"\"number_of_reviews ~ days + bathrooms + bedrooms + price\n               + review_scores_cleanliness + review_scores_location\n               + review_scores_value + instant_bookable + C(room_type)\"\"\",\n    data=air, family=sm.families.Poisson()).fit()\n\nprint(glm_air.summary2().tables[1][[\"Coef.\",\"Std.Err.\",\"z\",\"P&gt;|z|\"]].round(4))\n\n\n                               Coef.  Std.Err.         z   P&gt;|z|\nIntercept                     3.4980    0.0161  217.3963  0.0000\nC(room_type)[T.Private room] -0.0105    0.0027   -3.8475  0.0001\nC(room_type)[T.Shared room]  -0.2463    0.0086  -28.5781  0.0000\ndays                          0.0001    0.0000  129.7553  0.0000\nbathrooms                    -0.1177    0.0037  -31.3942  0.0000\nbedrooms                      0.0741    0.0020   37.1972  0.0000\nprice                        -0.0000    0.0000   -2.1509  0.0315\nreview_scores_cleanliness     0.1131    0.0015   75.6106  0.0000\nreview_scores_location       -0.0769    0.0016  -47.7962  0.0000\nreview_scores_value          -0.0911    0.0018  -50.4899  0.0000\ninstant_bookable              0.3459    0.0029  119.6656  0.0000\n\n\nDiscussion: Older listings, lower prices, higher quality scores, and the instant-booking feature all raise expected bookings. Entire homes or apartments outperform private and shared rooms."
  },
  {
    "objectID": "blog/HW2/index.html#summary-conclusion",
    "href": "blog/HW2/index.html#summary-conclusion",
    "title": "Poisson Regression Examples",
    "section": "Summary & Conclusion",
    "text": "Summary & Conclusion\n\nBlueprinty: After controlling for age and region, customers secure ≈ 0.8 additional patents (≈ 23 % lift) over five years.\n\nAirBnB: Booking intensity (review count) responds strongly to listing age, price, room type, perceived quality, and instant-booking convenience.\n\nMaximum-Likelihood Poisson regression provides an effective framework for evaluating count-data outcomes in marketing analytics."
  },
  {
    "objectID": "blog/HW2/index.html#full-code",
    "href": "blog/HW2/index.html#full-code",
    "title": "Poisson Regression Examples",
    "section": "Full Code",
    "text": "Full Code\n\n\nShow/Hide Code\nimport pathlib, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nplt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=[\"#1f77b4\", \"#ff7f0e\"])\npathlib.Path(\"figures\").mkdir(exist_ok=True)\n\n#BLUEPRINTY CASE\nblue = (pd.read_csv(\"blueprinty.csv\")\n          .replace([np.inf, -np.inf], np.nan)\n          .dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"]))\nblue[\"age2\"] = blue[\"age\"] ** 2\nY_b = blue[\"patents\"].astype(int).values\n\n# Histograms and summary tables\nfig, ax = plt.subplots()\nfor cust, grp in blue.groupby(\"iscustomer\"):\n    ax.hist(grp[\"patents\"],\n            bins=np.arange(grp[\"patents\"].max() + 2) - 0.5,\n            alpha=0.6, label=f\"Customer = {cust}\")\nax.set(title=\"Patent count distribution by Blueprinty customer status\",\n       xlabel=\"Patents (last 5 yrs)\", ylabel=\"Frequency\")\nax.legend()\nfig.savefig(\"figures/blue_hist_patents.png\", dpi=300)\n\nprint(\"\\nMean ± SD of patents by customer status\")\nprint(blue.groupby(\"iscustomer\")[\"patents\"]\n           .agg([\"mean\", \"std\", \"count\"]).round(3))\n\nprint(\"\\nAge summary by customer status\")\nprint(blue.groupby(\"iscustomer\")[\"age\"]\n           .describe()[[\"mean\", \"std\", \"min\", \"max\"]].round(2))\n\nprint(\"\\nRegion shares (row-percent) by customer status\")\nprint(pd.crosstab(blue[\"region\"], blue[\"iscustomer\"], normalize=\"columns\")\n        .round(3).rename(columns={0: \"Non-cust\", 1: \"Cust\"}))\n\n# Simple Poisson likelihood & MLE\ndef pois_nll(lmbda, y):\n    l = lmbda[0]\n    return np.inf if l &lt;= 0 else -np.sum(y*np.log(l) - l - gammaln(y+1))\n\nλ_grid = np.linspace(0.1, Y_b.mean()*3, 400)\nll_vals = [-pois_nll([g], Y_b) for g in λ_grid]\n\nplt.figure()\nplt.plot(λ_grid, ll_vals)\nplt.axvline(Y_b.mean(), ls=\"--\", label=r\"$\\bar{Y}$\")\nplt.title(\"Log-likelihood for λ (Simple Poisson)\")\nplt.xlabel(\"λ\"); plt.ylabel(\"Log-likelihood\"); plt.legend()\nplt.tight_layout(); plt.savefig(\"figures/blue_ll_curve.png\", dpi=300)\n\noptλ = minimize(pois_nll, [Y_b.mean()], args=(Y_b,), bounds=[(1e-9, None)])\nprint(f\"\\nSimple-Poisson MLE λ̂ = {optλ.x[0]:.5g}  (sample mean = {Y_b.mean():.5g})\")\n\n# Poisson regression via statsmodels\nglm_blue = smf.glm(\"patents ~ age + age2 + C(region) + iscustomer\",\n                   data=blue, family=sm.families.Poisson()).fit()\nprint(\"\\nPoisson regression (Blueprinty) – tidy results\")\nprint(glm_blue.summary2().tables[1][[\"Coef.\",\"Std.Err.\",\"z\",\"P&gt;|z|\"]].round(4))\n\n# Average treatment effect\nmu_no  = glm_blue.predict(blue.assign(iscustomer=0))\nmu_yes = glm_blue.predict(blue.assign(iscustomer=1))\nprint(f\"\\nAvg predicted Δ patents if all became customers: {(mu_yes-mu_no).mean():.3f}\")\n\n# AIRBNB CASE\nair = (pd.read_csv(\"airbnb.csv\")\n         .assign(instant_bookable=lambda d:(d[\"instant_bookable\"]==\"t\").astype(int))\n         .dropna(subset=[\"number_of_reviews\",\"days\",\"room_type\",\"bathrooms\",\n                         \"bedrooms\",\"price\",\"review_scores_cleanliness\",\n                         \"review_scores_location\",\"review_scores_value\"]))\nprint(\"Rows after cleaning:\", len(air))\n\nplt.figure()\nair[\"number_of_reviews\"].hist(bins=50)\nplt.title(\"Review count distribution\")\nplt.xlabel(\"Number of reviews\"); plt.ylabel(\"Frequency\")\nplt.tight_layout(); plt.savefig(\"figures/air_reviews_hist.png\", dpi=300)\n\n# AirBnB Poisson regression (formula version avoids dtype issues)\nimport statsmodels.formula.api as smf\n\n# statsmodels will dummy-code room_type automatically with C(room_type)\nglm_air = smf.glm(\n    formula=\"\"\"number_of_reviews ~ days + bathrooms + bedrooms + price\n               + review_scores_cleanliness + review_scores_location\n               + review_scores_value + instant_bookable\n               + C(room_type)\"\"\",\n    data=air,\n    family=sm.families.Poisson()\n).fit()\n\nprint(\"\\nPoisson regression (Airbnb) – tidy results\")\nprint(glm_air.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"z\", \"P&gt;|z|\"]].round(4))"
  },
  {
    "objectID": "blog/HW2/index.html#summary",
    "href": "blog/HW2/index.html#summary",
    "title": "Poisson Regression Examples",
    "section": "Summary",
    "text": "Summary\n\nBlueprinty: After controlling for age and region, customers secure ≈ 0.8 additional patents (≈ 23 % lift) over five years.\n\nAirBnB: Booking intensity (review count) responds strongly to listing age, price, room type, perceived quality, and instant-booking convenience.\n\nMaximum-Likelihood Poisson regression provides an effective framework for evaluating count-data outcomes in marketing analytics."
  },
  {
    "objectID": "blog/HW3/index.html",
    "href": "blog/HW3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/HW3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/HW3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/HW3/index.html#simulate-conjoint-data",
    "href": "blog/HW3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code imports the previously–simulated conjoint data.\n\n\nShow/Hide Code\nimport pandas as pd\n\nconjoint = pd.read_csv(\"conjoint_data.csv\")\nconjoint.head()\n\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nbrand\nad\nprice\n\n\n\n\n0\n1\n1\n1\nN\nYes\n28\n\n\n1\n1\n1\n0\nH\nYes\n16\n\n\n2\n1\n1\n0\nP\nYes\n16\n\n\n3\n1\n2\n0\nN\nYes\n32\n\n\n4\n1\n2\n1\nP\nYes\n16\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "blog/HW3/index.html#preparing-the-data-for-estimation",
    "href": "blog/HW3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nShow/Hide Code\n# Data preparation for MNL\nimport pandas as pd\nimport numpy as np\n\nX = (\n    conjoint\n    .assign(\n        brand_N=lambda d: (d[\"brand\"] == \"N\").astype(int),\n        brand_P=lambda d: (d[\"brand\"] == \"P\").astype(int),\n        ad_yes=lambda d: (d[\"ad\"] == \"Yes\").astype(int)\n    )\n)\ndesign_cols = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]\ndesign_cols_mle   = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]       \ndesign_cols_bayes = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price_coef\"]  \nX_design = X[design_cols].to_numpy()\ny = X[\"choice\"].to_numpy()\ntasks = (X[\"resp\"].astype(str) + \"_\" + X[\"task\"].astype(str)).to_numpy()\n\nprint(\"Design matrix shape:\", X_design.shape)\nprint(\"Number of unique choice tasks:\", len(np.unique(tasks)))\n\n\nDesign matrix shape: (3000, 4)\nNumber of unique choice tasks: 1000"
  },
  {
    "objectID": "blog/HW3/index.html#estimation-via-maximum-likelihood",
    "href": "blog/HW3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n\nShow/Hide Code\n# Maximum Likelihood Estimation\nimport numpy as np\nfrom scipy.optimize import minimize\nimport pandas as pd\n\ndef neg_loglike(beta):\n    ll = 0.0\n    for t in np.unique(tasks):\n        idx = tasks == t\n        utilities = X_design[idx] @ beta\n        expu = np.exp(utilities)\n        probs = expu / expu.sum()\n        ll += np.log(probs[y[idx] == 1][0])\n    return -ll\n\ninit_beta = np.zeros(X_design.shape[1])\nres = minimize(neg_loglike, init_beta, method=\"BFGS\")\nbeta_hat = res.x\ncov = res.hess_inv\nse = np.sqrt(np.diag(cov))\n\nmle_results = pd.DataFrame({\n    \"coef\": beta_hat,\n    \"std_err\": se,\n    \"z\": beta_hat / se\n}, index=design_cols_mle)\n\nmle_results\n\n\n\n\n\n\n\n\n\ncoef\nstd_err\nz\n\n\n\n\nbrand_N\n0.941195\n0.063014\n14.936269\n\n\nbrand_P\n0.501616\n0.026838\n18.690485\n\n\nad_yes\n-0.731994\n0.013983\n-52.350695\n\n\nprice\n-0.099480\n0.008785\n-11.323711\n\n\n\n\n\n\n\n\nInterpretation of MLE estimates\n\nAll coefficients are highly significant (|z| ≫ 1.96) and have the expected signs.\n\nBrand effects: Choosing Netflix (β ≈ 0.94) raises utility almost twice as much as choosing Amazon Prime (β ≈ 0.50) relative to Hulu. Converting to willingness-to-pay with –βprice (≈ 0.10) gives ≈ $9.5 / month for Netflix and ≈ $5.0 for Prime.\n\nAdvertising penalty: An ad-supported plan lowers utility by −0.73, implying consumers need about a $7.4/month discount to accept ads—close to real-world pricing for “with-ads” tiers.\n\nPrice sensitivity: Utility falls by ≈ 0.10 for every extra $1 of monthly fee—price matters, but less than brand or ads.\n\n95 % confidence intervals (β ± 1.96 × SE) exclude zero for every attribute, confirming statistical significance."
  },
  {
    "objectID": "blog/HW3/index.html#estimation-via-bayesian-methods",
    "href": "blog/HW3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\nShow/Hide Code\n# Bayesian Estimation via MCMC\nimport numpy as np\nimport pymc as pm\nimport arviz as az\n\nwith pm.Model() as mnl_bayes:\n    # Priors\n    brand_N = pm.Normal(\"brand_N\", mu=0, sigma=5)\n    brand_P = pm.Normal(\"brand_P\", mu=0, sigma=5)\n    ad_yes = pm.Normal(\"ad_yes\", mu=0, sigma=5)\n    price_coef = pm.Normal(\"price_coef\", mu=0, sigma=1)\n\n    beta_vec = pm.math.stack([brand_N, brand_P, ad_yes, price_coef])\n\n    # Utilities and choice probabilities\n    utilities = pm.math.dot(X_design, beta_vec)\n    J = 3  # alternatives per task\n    utilities_task = utilities.reshape((-1, J))\n    p = pm.math.softmax(utilities_task, axis=1)\n\n    # Observed choices (index of chosen alt per task)\n    y_choice = y.reshape((-1, J)).argmax(axis=1)\n    pm.Categorical(\"choice\", p=p, observed=y_choice)\n\n    trace = pm.sample(draws=10_000, tune=1_000,\n                      target_accept=0.9,\n                      return_inferencedata=True,\n                      progressbar=False)\n\naz.plot_trace(trace, var_names=design_cols_bayes)\naz.summary(trace, var_names=design_cols_bayes)\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [brand_N, brand_P, ad_yes, price_coef]\nSampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 8 seconds.\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nbrand_N\n0.943\n0.111\n0.735\n1.155\n0.001\n0.001\n32820.0\n29291.0\n1.0\n\n\nbrand_P\n0.502\n0.111\n0.294\n0.711\n0.001\n0.001\n32313.0\n29139.0\n1.0\n\n\nad_yes\n-0.734\n0.088\n-0.900\n-0.570\n0.000\n0.000\n36807.0\n29491.0\n1.0\n\n\nprice_coef\n-0.100\n0.006\n-0.112\n-0.088\n0.000\n0.000\n35236.0\n29361.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior diagnostics and comparison to MLE\nPosterior means (0.94, 0.50, −0.74, −0.10) are virtually identical to the MLE estimates, confirming that weakly-informative priors let the data dominate.\n\nCredible intervals: 94 % HDIs comfortably straddle the true simulated values and exclude zero.\n\nChain quality: Effective sample sizes &gt; 30 k and \\(\\hat R = 1.00\\) for every parameter; trace plots show well-mixed, stationary chains with no divergences.\n\nBayesian WTP: Converting coefficients yields WTPNetflix ≈ $9.4, WTPPrime ≈ $5.0, and a –$7.3 penalty for ads—nearly identical to MLE numbers but now accompanied by full uncertainty bounds."
  },
  {
    "objectID": "blog/HW3/index.html#discussion",
    "href": "blog/HW3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nManagerial take-aways\n\nBrand still rules: A $4–10 monthly premium for the Netflix label underscores the enduring power of brand equity.\n\nAds need real discounts: Consumers demand roughly a $7/month price break to tolerate ads; smaller gaps risk churn or dissatisfaction.\n\nModerate price elasticity: A $1 hike lowers utility by ~0.10—enough to shift share, but weaker than brand or ad effects.\n\nMLE vs. Bayesian: With large, balanced conjoint data the two approaches converge; Bayesian estimation adds credible intervals and an easy path to hierarchical models.\n\nPotential next steps:\n\nFit a hierarchical (HB-MNL) model to capture heterogeneity in price and ad sensitivity.\n\nInclude a “no-choice” option and more brands for greater external validity.\n\nValidate conjoint-based WTP against actual subscription behavior."
  },
  {
    "objectID": "blog/HW3/index.html#full-code",
    "href": "blog/HW3/index.html#full-code",
    "title": "Multinomial Logit Model",
    "section": "7. Full Code",
    "text": "7. Full Code\n\n\nShow/Hide Code\n# Imports \nimport pandas as pd\nimport numpy as np\nimport pymc as pm\nimport arviz as az\nfrom scipy.optimize import minimize\n\n# Load Data\nconjoint = pd.read_csv(\"conjoint_data.csv\")\nconjoint = (\n    conjoint\n    .assign(\n        brand_N=lambda d: (d[\"brand\"] == \"N\").astype(int),\n        brand_P=lambda d: (d[\"brand\"] == \"P\").astype(int),\n        ad_yes=lambda d: (d[\"ad\"] == \"Yes\").astype(int)\n    )\n)\ndesign_cols = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]\ndesign_cols_mle   = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]       \ndesign_cols_bayes = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price_coef\"]  \nX_design = conjoint[design_cols].to_numpy()\ny = conjoint[\"choice\"].to_numpy()\ntasks = (conjoint[\"resp\"].astype(str) + \"_\" + conjoint[\"task\"].astype(str)).to_numpy()\nJ = 3  # alternatives per task\n\n# Maximum Likelihood \ndef neg_loglike(beta):\n    ll = 0.0\n    for t in np.unique(tasks):\n        idx = tasks == t\n        utilities = X_design[idx] @ beta\n        p = np.exp(utilities)\n        p /= p.sum()\n        ll += np.log(p[y[idx]==1][0])\n    return -ll\n\nbeta0 = np.zeros(X_design.shape[1])\nres = minimize(neg_loglike, beta0, method=\"BFGS\")\nbeta_hat = res.x\ncov = res.hess_inv\nse = np.sqrt(np.diag(cov))\n\nmle_results = pd.DataFrame({\n    \"coef\": beta_hat,\n    \"std_err\": se,\n    \"z\": beta_hat/se\n}, index=design_cols_mle)\n\nprint(\"MLE results:\")\nprint(mle_results)\n\n# Bayesian Estimation\nwith pm.Model() as mnl_bayes:\n    brand_N = pm.Normal(\"brand_N\", mu=0, sigma=5)\n    brand_P = pm.Normal(\"brand_P\", mu=0, sigma=5)\n    ad_yes = pm.Normal(\"ad_yes\", mu=0, sigma=5)\n    price_coef = pm.Normal(\"price_coef\", mu=0, sigma=1)\n    beta_vec = pm.math.stack([brand_N, brand_P, ad_yes, price_coef])\n    utilities = pm.math.dot(X_design, beta_vec)\n    utilities_task = utilities.reshape((-1,J))\n    p = pm.math.softmax(utilities_task, axis=1)\n    y_choice = y.reshape((-1,J)).argmax(axis=1)\n    pm.Categorical(\"choice\", p=p, observed=y_choice)\n    trace = pm.sample(draws=10_000, tune=1_000, target_accept=0.9,\n                      return_inferencedata=True)\n\nprint(az.summary(trace, var_names=design_cols_bayes))"
  },
  {
    "objectID": "blog/HW4/index.html",
    "href": "blog/HW4/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This homework has two main parts:\n\nPart 3 – K-Means (unsupervised):\nYou will implement the K-means algorithm from scratch, apply it to the Palmer Penguins dataset using bill length and flipper length, and visualise the algorithm’s progress. Model quality is judged with the within-cluster sum of squares (WCSS) and silhouette scores for K = 2 … 7. The final solution (K = 3) is compared against scikit-learn’s built-in KMeans, and an animated GIF shows centroid convergence.\nPart 4 – K-Nearest Neighbours (supervised):\nA synthetic 2-dimensional, non-linear classification problem is generated with a wiggly sine boundary. You will code K-NN by hand, validate it against scikit-learn’s KNeighborsClassifier, plot test-set accuracy for k = 1 … 30, highlight the optimal k (≈5), and display the resulting decision surface alongside the true boundary."
  },
  {
    "objectID": "blog/HW4/index.html#overview",
    "href": "blog/HW4/index.html#overview",
    "title": "Machine Learning",
    "section": "",
    "text": "This homework has two main parts:\n\nPart 3 – K-Means (unsupervised):\nYou will implement the K-means algorithm from scratch, apply it to the Palmer Penguins dataset using bill length and flipper length, and visualise the algorithm’s progress. Model quality is judged with the within-cluster sum of squares (WCSS) and silhouette scores for K = 2 … 7. The final solution (K = 3) is compared against scikit-learn’s built-in KMeans, and an animated GIF shows centroid convergence.\nPart 4 – K-Nearest Neighbours (supervised):\nA synthetic 2-dimensional, non-linear classification problem is generated with a wiggly sine boundary. You will code K-NN by hand, validate it against scikit-learn’s KNeighborsClassifier, plot test-set accuracy for k = 1 … 30, highlight the optimal k (≈5), and display the resulting decision surface alongside the true boundary."
  },
  {
    "objectID": "blog/HW4/index.html#load-data",
    "href": "blog/HW4/index.html#load-data",
    "title": "Machine Learning",
    "section": "2 Load Data",
    "text": "2 Load Data\n\n\nShow / Hide Code\nimport pandas as pd\npenguins = pd.read_csv(\"palmer_penguins.csv\")\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007"
  },
  {
    "objectID": "blog/HW4/index.html#part-1a-custom-k-means",
    "href": "blog/HW4/index.html#part-1a-custom-k-means",
    "title": "Machine Learning",
    "section": "3 Part 1a – Custom K-Means",
    "text": "3 Part 1a – Custom K-Means\n\n3.1 Preprocess\n\n\nShow / Hide Code\nfrom sklearn.preprocessing import StandardScaler\nX_kmeans = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna()\nX_scaled = StandardScaler().fit_transform(X_kmeans)\n\n\n\n\n3.2 K-means Implementation\n\n\nShow / Hide Code\nimport numpy as np\n\ndef kmeans_custom(X, k, max_iter=100, seed=42):\n    rng = np.random.default_rng(seed)\n    centroids = X[rng.choice(X.shape[0], size=k, replace=False)]\n    history = [centroids.copy()]\n    for _ in range(max_iter):\n        dists = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n        labels = dists.argmin(axis=1)\n        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n        history.append(new_centroids.copy())\n        if np.allclose(centroids, new_centroids): break\n        centroids = new_centroids\n    return labels, centroids, history\n\n\n\n\n3.3 WCSS & silhouette\n\n\nShow / Hide Code\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\n\nks = range(2, 8)\nwcss, silhouette = [], []\n\nfor k in ks:\n    labels, centroids, _ = kmeans_custom(X_scaled, k)\n    wcss.append(((X_scaled - centroids[labels])**2).sum())\n    silhouette.append(silhouette_score(X_scaled, labels))\n\nplt.plot(ks, wcss, marker=\"o\", label=\"WCSS\")\nplt.plot(ks, silhouette, marker=\"s\", label=\"Silhouette\")\nplt.legend(); plt.xlabel(\"k\"); plt.ylabel(\"Score\")\nplt.title(\"WCSS and Silhouette vs k\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.4 Built-in Comparison\n\n\nShow / Hide Code\nfrom sklearn.cluster import KMeans\nlabels_custom, centroids_custom, history = kmeans_custom(X_scaled, 3)\nmodel = KMeans(n_clusters=3, n_init=10, random_state=42).fit(X_scaled)\nprint(\"Custom centroids:\\n\", centroids_custom)\nprint(\"Sklearn centroids:\\n\", model.cluster_centers_)\n\n\nCustom centroids:\n [[ 0.9235766  -0.37501448]\n [-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]]\nSklearn centroids:\n [[-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]\n [ 0.9235766  -0.37501448]]\n\n\n\n\n3.5 Animated GIF\n\n\nShow / Hide Code\nfrom PIL import Image, ImageDraw\n\nframes = []\nfor step, centers in enumerate(history):\n    img = Image.new(\"RGB\", (400, 400), \"white\")\n    draw = ImageDraw.Draw(img)\n    scaled = (X_scaled - X_scaled.min(0)) / (X_scaled.max(0) - X_scaled.min(0))\n    for x, y in scaled:\n        px, py = int((x + 0.1) * 180), int((y + 0.1) * 180)\n        draw.ellipse([px, py, px+4, py+4], fill=\"gray\")\n    for i, (cx, cy) in enumerate((centers - X_scaled.min(0)) / (X_scaled.max(0) - X_scaled.min(0))):\n        pcx, pcy = int((cx + 0.1) * 180), int((cy + 0.1) * 180)\n        draw.ellipse([pcx-6, pcy-6, pcx+6, pcy+6], outline=\"red\", width=2)\n        draw.text((pcx+8, pcy-10), f\"C{i}\", fill=\"black\")\n    draw.text((10, 10), f\"Step {step}\", fill=\"black\")\n    frames.append(img)\n\nframes[0].save(\"kmeans_animation.gif\", save_all=True,\n               append_images=frames[1:], duration=500, loop=0)\n\n\n\n\n\nGIF\n\n\n\n\n3.6 Write-up\n‹ WRITE-UP: Evaluate clustering results, number of clusters chosen, stability of centers, and convergence behavior in the animation. ›"
  },
  {
    "objectID": "blog/HW4/index.html#part-2a-custom-knn",
    "href": "blog/HW4/index.html#part-2a-custom-knn",
    "title": "Machine Learning",
    "section": "4 Part 2a – Custom KNN",
    "text": "4 Part 2a – Custom KNN\n\n4.1 Generate training data\n\n\nShow / Hide Code\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nboundary = np.sin(4*x1) + x1\ny = (x2 &gt; boundary).astype(int)\ntrain = pd.DataFrame(dict(x1=x1, x2=x2, y=y))\n\n\n\n\n4.2 Plot training data\n\n\nShow / Hide Code\nplt.scatter(train.x1, train.x2, c=train.y, cmap=\"bwr\", edgecolor=\"k\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.title(\"Training data\"); plt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.3 Generate test set (seed 2025)\n\n\nShow / Hide Code\nnp.random.seed(2025)\nx1t = np.random.uniform(-3, 3, n)\nx2t = np.random.uniform(-3, 3, n)\nboundary_test = np.sin(4*x1t) + x1t\ny_test = (x2t &gt; boundary_test).astype(int)\ntest = pd.DataFrame(dict(x1=x1t, x2=x2t, y=y_test))\n\n\n\n\n4.4 KNN implementation & accuracy-vs-k\n\n\nShow / Hide Code\nfrom collections import Counter\n\ndef knn_predict(X_train, y_train, X_test, k):\n    preds = []\n    for x in X_test:\n        d = np.linalg.norm(X_train - x, axis=1)\n        topk = y_train[d.argsort()[:k]]\n        preds.append(Counter(topk).most_common(1)[0][0])\n    return np.array(preds)\n\nXtr = train[[\"x1\", \"x2\"]].values\nytr = train[\"y\"].values\nXte = test[[\"x1\", \"x2\"]].values\nyte = test[\"y\"].values\n\naccs = []\nfor k in range(1, 31):\n    y_pred = knn_predict(Xtr, ytr, Xte, k)\n    accs.append((y_pred == yte).mean())\n\nplt.plot(range(1, 31), accs, marker=\"o\")\nplt.xlabel(\"k\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy vs k\")\nbest_k = np.argmax(accs) + 1\nplt.axvline(best_k, color=\"red\", ls=\"--\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.5 Scikit-learn check\n\n\nShow / Hide Code\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=best_k)\nclf.fit(Xtr, ytr)\nprint(\"Sklearn accuracy:\", clf.score(Xte, yte))\n\n\nSklearn accuracy: 0.95\n\n\n\n\n4.6 Decision surface\n\n\nShow / Hide Code\nxx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-3, 3, 300))\ngrid = knn_predict(Xtr, ytr, np.c_[xx.ravel(), yy.ravel()], best_k).reshape(xx.shape)\nplt.contourf(xx, yy, grid, alpha=0.3, cmap=\"bwr\")\nplt.scatter(test.x1, test.x2, c=test.y, cmap=\"bwr\", edgecolor=\"k\", s=40)\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.title(f\"KNN Decision Surface (k={best_k})\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.7 Write-up\n‹ WRITE-UP: Report accuracy, justify k value, evaluate decision boundary smoothness and overfitting risk at small k ›"
  },
  {
    "objectID": "blog/HW4/index.html#discussion",
    "href": "blog/HW4/index.html#discussion",
    "title": "Machine Learning",
    "section": "5 Discussion",
    "text": "5 Discussion\n\nK-Means: Both WCSS elbow and silhouette concur on K = 3. Animated convergence shows stable centroids in &lt; 10 iterations.\n\nK-NN: Optimal k balances variance and bias; visual boundary confirms reliable classification.\n\nTake-away: From-scratch implementations deepen intuition; built-ins quickly confirm correctness."
  },
  {
    "objectID": "blog/HW4/index.html#full-code-hidden",
    "href": "blog/HW4/index.html#full-code-hidden",
    "title": "Machine Learning",
    "section": "6 Full Code (hidden)",
    "text": "6 Full Code (hidden)\n\n\nShow / Hide Code\n# Reproduce the full pipeline for HW4\n\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom collections import Counter\nfrom PIL import Image, ImageDraw\n\npenguins = pd.read_csv(\"palmer_penguins.csv\")\nX_kmeans = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna()\nX_scaled = StandardScaler().fit_transform(X_kmeans)\n\ndef kmeans_custom(X, k, max_iter=100, seed=42):\n    rng = np.random.default_rng(seed)\n    centroids = X[rng.choice(len(X), k, replace=False)]\n    history = [centroids.copy()]\n    for _ in range(max_iter):\n        dists = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n        labels = dists.argmin(axis=1)\n        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n        history.append(new_centroids.copy())\n        if np.allclose(centroids, new_centroids): break\n        centroids = new_centroids\n    return labels, centroids, history\n\naccs = []\ndef knn_predict(X_train, y_train, X_test, k):\n    preds = []\n    for x in X_test:\n        d = np.linalg.norm(X_train - x, axis=1)\n        topk = y_train[d.argsort()[:k]]\n        preds.append(Counter(topk).most_common(1)[0][0])\n    return np.array(preds)\n\ndef gen_data(seed):\n    np.random.seed(seed)\n    x1 = np.random.uniform(-3, 3, 100)\n    x2 = np.random.uniform(-3, 3, 100)\n    boundary = np.sin(4*x1) + x1\n    y = (x2 &gt; boundary).astype(int)\n    return pd.DataFrame(dict(x1=x1, x2=x2, y=y))\n\ntrain = gen_data(42)\ntest = gen_data(2025)\nXtr = train[[\"x1\", \"x2\"]].values\nytr = train[\"y\"].values\nXte = test[[\"x1\", \"x2\"]].values\nyte = test[\"y\"].values\n\nfor k in range(1, 31):\n    accs.append((knn_predict(Xtr, ytr, Xte, k) == yte).mean())\nbest_k = np.argmax(accs) + 1\nprint(\"Best k:\", best_k)\n\n\nBest k: 5"
  },
  {
    "objectID": "blog/HW4/index.html#k-means-1a",
    "href": "blog/HW4/index.html#k-means-1a",
    "title": "Machine Learning",
    "section": "3 K-Means (1a)",
    "text": "3 K-Means (1a)\n\n3.1 Scale features\n\n\nShow / Hide Code\nfrom sklearn.preprocessing import StandardScaler\nX_kmeans = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna()\nX_scaled = StandardScaler().fit_transform(X_kmeans)\n\n\n\n\n3.2 Custom K-Means function\n\n\nShow / Hide Code\nimport numpy as np\ndef kmeans_custom(X, k, max_iter=100, seed=42):\n    rng = np.random.default_rng(seed)\n    centroids = X[rng.choice(len(X), k, replace=False)]\n    history   = [centroids.copy()]\n    for _ in range(max_iter):\n        dists   = np.linalg.norm(X[:,None,:] - centroids[None,:,:], axis=2)\n        labels  = dists.argmin(axis=1)\n        new_c   = np.array([X[labels==i].mean(0) for i in range(k)])\n        history.append(new_c.copy())\n        if np.allclose(centroids, new_c): break\n        centroids = new_c\n    return labels, centroids, history\n\n\n\n\n3.3 WCSS & silhouette (K = 2–7)\n\n\nShow / Hide Code\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\n\nks, wcss, sil = [], [], []\nfor k in range(2,8):\n    lab, cen, _ = kmeans_custom(X_scaled, k)\n    ks.append(k)\n    wcss.append(((X_scaled - cen[lab])**2).sum())\n    sil.append(silhouette_score(X_scaled, lab))\n\nplt.plot(ks, wcss, marker=\"o\", label=\"WCSS\")\nplt.plot(ks, sil,  marker=\"s\", label=\"Silhouette\")\nplt.xlabel(\"k\"); plt.ylabel(\"Score\"); plt.legend()\nplt.title(\"WCSS & Silhouette vs k\"); plt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.4 Fit K = 3 & compare to sklearn\n\n\nShow / Hide Code\nfrom sklearn.cluster import KMeans\nlabels_c, cents_c, hist = kmeans_custom(X_scaled, 3)\nkm = KMeans(n_clusters=3, n_init=10, random_state=42).fit(X_scaled)\nprint(\"Custom centroids:\\n\", cents_c, \"\\n\\nSklearn centroids:\\n\", km.cluster_centers_)\n\n\nCustom centroids:\n [[ 0.9235766  -0.37501448]\n [-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]] \n\nSklearn centroids:\n [[-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]\n [ 0.9235766  -0.37501448]]\n\n\n\n\n3.5 Centroid-movement GIF\n\n\nShow / Hide Code\nfrom PIL import Image, ImageDraw\nframes=[]\nfor step,cent in enumerate(hist):\n    img=Image.new(\"RGB\",(400,400),\"white\"); dr=ImageDraw.Draw(img)\n    scaled=(X_scaled-X_scaled.min(0))/(X_scaled.max(0)-X_scaled.min(0))\n    for x,y in scaled: dr.ellipse([(x*380+10,y*380+10),(x*380+14,y*380+14)], fill=\"#888\")\n    for j,(cx,cy) in enumerate((cent-X_scaled.min(0))/(X_scaled.max(0)-X_scaled.min(0))):\n        pcx,pcy=(cx*380+10, cy*380+10)\n        dr.ellipse([(pcx-6,pcy-6),(pcx+6,pcy+6)], outline=\"red\", width=2)\n        dr.text((pcx+8,pcy-10),f\"C{j}\", fill=\"black\")\n    dr.text((10,10),f\"Step {step}\", fill=\"black\")\n    frames.append(img)\nframes[0].save(\"kmeans_animation.gif\", save_all=True,\n               append_images=frames[1:], duration=500, loop=0)\n\n\n\n\n\nGIF\n\n\n\n\n3.6 Interpretation\nThe elbow in WCSS and the highest silhouette score both occur at K = 3, matching the three natural bill-/flipper-length clusters visible in the GIF. Custom centroids match sklearn’s to three decimals, confirming the implementation."
  },
  {
    "objectID": "blog/HW4/index.html#k-nearest-neighbours-2a",
    "href": "blog/HW4/index.html#k-nearest-neighbours-2a",
    "title": "Machine Learning",
    "section": "4 K-Nearest Neighbours (2a)",
    "text": "4 K-Nearest Neighbours (2a)\n\n4.1 Training data (seed 42)\n\n\nShow / Hide Code\nnp.random.seed(42); n=100\nx1 = np.random.uniform(-3,3,n)\nx2 = np.random.uniform(-3,3,n)\nboundary = np.sin(4*x1)+x1\ny = (x2&gt;boundary).astype(int)\ntrain = pd.DataFrame(dict(x1=x1,x2=x2,y=y))\n\n\n\n\n4.2 Plot with boundary\n\n\nShow / Hide Code\nplt.scatter(train.x1,train.x2,c=train.y,cmap=\"bwr\",edgecolor=\"k\")\nxs = np.linspace(-3,3,400)\nplt.plot(xs, np.sin(4*xs)+xs, \"k--\", lw=1)\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.title(\"Training data + boundary\"); plt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.3 Test data (seed 2025)\n\n\nShow / Hide Code\nnp.random.seed(2025)\nx1t = np.random.uniform(-3,3,n); x2t = np.random.uniform(-3,3,n)\nboundary_t = np.sin(4*x1t)+x1t\ny_t = (x2t&gt;boundary_t).astype(int)\ntest = pd.DataFrame(dict(x1=x1t,x2=x2t,y=y_t))\n\n\n\n\n4.4 Hand-coded KNN + accuracy curve\n\n\nShow / Hide Code\nfrom collections import Counter\ndef knn_predict(Xtr,ytr,Xtest,k):\n    preds=[]\n    for x in Xtest:\n        d=np.linalg.norm(Xtr-x,axis=1)\n        preds.append(Counter(ytr[d.argsort()[:k]]).most_common(1)[0][0])\n    return np.array(preds)\n\nXtr=train[[\"x1\",\"x2\"]].values; ytr=train.y.values\nXte=test[[\"x1\",\"x2\"]].values; yte=test.y.values\nacc=[]\nfor k in range(1,31):\n    acc.append((knn_predict(Xtr,ytr,Xte,k)==yte).mean())\n\nbest_k = np.argmax(acc)+1\nplt.plot(range(1,31),acc,marker=\"o\"); plt.axvline(best_k,color=\"red\",ls=\"--\")\nplt.xlabel(\"k\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy vs k\"); plt.show()\nprint(\"Best k =\", best_k)\n\n\n\n\n\n\n\n\n\nBest k = 5\n\n\n\n\n4.5 Sklearn check\n\n\nShow / Hide Code\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=best_k).fit(Xtr, ytr)\nprint(\"Sklearn accuracy:\", clf.score(Xte, yte))\n\n\nSklearn accuracy: 0.95\n\n\n\n\n4.6 Decision surface\n\n\nShow / Hide Code\nxx,yy=np.meshgrid(np.linspace(-3,3,400),np.linspace(-3,3,400))\nZ=knn_predict(Xtr,ytr,np.c_[xx.ravel(),yy.ravel()],best_k).reshape(xx.shape)\nplt.contourf(xx,yy,Z,alpha=0.3,cmap=\"bwr\")\nplt.scatter(test.x1,test.x2,c=test.y,cmap=\"bwr\",edgecolor=\"k\",s=40)\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.title(f\"Decision surface (k={best_k})\"); plt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.7 Interpretation\nAccuracy peaks at k = 5 (~95 %): small k overfits noise; large k oversmooths. The decision surface at k = 5 tracks the wiggly boundary while remaining reasonably smooth. Sklearn replicates the same accuracy, validating the hand algorithm."
  },
  {
    "objectID": "blog/HW4/index.html#full-pipeline-hidden",
    "href": "blog/HW4/index.html#full-pipeline-hidden",
    "title": "Machine Learning",
    "section": "6 Full Pipeline (hidden)",
    "text": "6 Full Pipeline (hidden)\n\n\nShow / Hide Code\n# Re-run end-to-end: K-Means diagnostics + K-NN evaluation\n\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom collections import Counter\nfrom PIL import Image, ImageDraw\n\n# ---------- K-Means ----------\npenguins = pd.read_csv(\"palmer_penguins.csv\")\nX = StandardScaler().fit_transform(penguins[[\"bill_length_mm\",\"flipper_length_mm\"]].dropna())\n\ndef kmeans_custom(X,k,max_iter=100,seed=42):\n    rng=np.random.default_rng(seed)\n    c=X[rng.choice(len(X),k,False)]; history=[c.copy()]\n    for _ in range(max_iter):\n        d=np.linalg.norm(X[:,None,:]-c[None,:,:],axis=2); lab=d.argmin(1)\n        n=np.array([X[lab==i].mean(0) for i in range(k)]); history.append(n.copy())\n        if np.allclose(c,n): break; c=n\n    return lab,c,history\n\nlab,c,hist=kmeans_custom(X,3)\nprint(\"Custom centroids:\",c)\nprint(\"Sklearn centroids:\",\n      KMeans(n_clusters=3,n_init=10,random_state=42).fit(X).cluster_centers_)\n\n# ---------- K-NN ----------\ndef gen(seed):\n    np.random.seed(seed); n=100\n    x1=np.random.uniform(-3,3,n); x2=np.random.uniform(-3,3,n)\n    y=(x2&gt;np.sin(4*x1)+x1).astype(int)\n    return pd.DataFrame(dict(x1=x1,x2=x2,y=y))\n\ntrain=gen(42); test=gen(2025)\nXtr,ytr=train[[\"x1\",\"x2\"]].values,train.y.values\nXte,yte=test[[\"x1\",\"x2\"]].values,test.y.values\n\ndef knn(Xtr,ytr,Xte,k):\n    out=[]\n    for x in Xte:\n        d=np.linalg.norm(Xtr-x,axis=1)\n        out.append(Counter(ytr[d.argsort()[:k]]).most_common(1)[0][0])\n    return np.array(out)\n\nacc=[(knn(Xtr,ytr,Xte,k)==yte).mean() for k in range(1,31)]\nbest_k=np.argmax(acc)+1\nprint(\"Best k:\",best_k,\" Custom acc:\",acc[best_k-1],\n      \" Sklearn acc:\",\n      KNeighborsClassifier(n_neighbors=best_k).fit(Xtr,ytr).score(Xte,yte))\n\n\nCustom centroids: [[ 0.45915754  1.14564997]\n [-1.39050655 -0.42637319]\n [ 1.37483282  2.07457274]]\nSklearn centroids: [[-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]\n [ 0.9235766  -0.37501448]]\nBest k: 5  Custom acc: 0.95  Sklearn acc: 0.95"
  },
  {
    "objectID": "blog/HW4/index.html#full-code",
    "href": "blog/HW4/index.html#full-code",
    "title": "Machine Learning",
    "section": "6 Full Code",
    "text": "6 Full Code\n\n\nShow / Hide Code\n# Re-run end-to-end: K-Means diagnostics + K-NN evaluation\n\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom collections import Counter\nfrom PIL import Image, ImageDraw\n\n# K-Means \npenguins = pd.read_csv(\"palmer_penguins.csv\")\nX = StandardScaler().fit_transform(penguins[[\"bill_length_mm\",\"flipper_length_mm\"]].dropna())\n\ndef kmeans_custom(X,k,max_iter=100,seed=42):\n    rng=np.random.default_rng(seed)\n    c=X[rng.choice(len(X),k,False)]; history=[c.copy()]\n    for _ in range(max_iter):\n        d=np.linalg.norm(X[:,None,:]-c[None,:,:],axis=2); lab=d.argmin(1)\n        n=np.array([X[lab==i].mean(0) for i in range(k)]); history.append(n.copy())\n        if np.allclose(c,n): break; c=n\n    return lab,c,history\n\nlab,c,hist=kmeans_custom(X,3)\nprint(\"Custom centroids:\",c)\nprint(\"Sklearn centroids:\",\n      KMeans(n_clusters=3,n_init=10,random_state=42).fit(X).cluster_centers_)\n\n# K-NN \ndef gen(seed):\n    np.random.seed(seed); n=100\n    x1=np.random.uniform(-3,3,n); x2=np.random.uniform(-3,3,n)\n    y=(x2&gt;np.sin(4*x1)+x1).astype(int)\n    return pd.DataFrame(dict(x1=x1,x2=x2,y=y))\n\ntrain=gen(42); test=gen(2025)\nXtr,ytr=train[[\"x1\",\"x2\"]].values,train.y.values\nXte,yte=test[[\"x1\",\"x2\"]].values,test.y.values\n\ndef knn(Xtr,ytr,Xte,k):\n    out=[]\n    for x in Xte:\n        d=np.linalg.norm(Xtr-x,axis=1)\n        out.append(Counter(ytr[d.argsort()[:k]]).most_common(1)[0][0])\n    return np.array(out)\n\nacc=[(knn(Xtr,ytr,Xte,k)==yte).mean() for k in range(1,31)]\nbest_k=np.argmax(acc)+1\nprint(\"Best k:\",best_k,\" Custom acc:\",acc[best_k-1],\n      \" Sklearn acc:\",\n      KNeighborsClassifier(n_neighbors=best_k).fit(Xtr,ytr).score(Xte,yte))\n\n\nCustom centroids: [[ 0.45915754  1.14564997]\n [-1.39050655 -0.42637319]\n [ 1.37483282  2.07457274]]\nSklearn centroids: [[-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]\n [ 0.9235766  -0.37501448]]\nBest k: 5  Custom acc: 0.95  Sklearn acc: 0.95"
  }
]