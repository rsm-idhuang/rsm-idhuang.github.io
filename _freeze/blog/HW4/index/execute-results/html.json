{
  "hash": "de688969bbc1ae57710451d2185552a6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning\"\nauthor: \"Idris Huang\"\ndate: June 11, 2025\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show / Hide Code\"\n    toc: true\n    toc-depth: 2\nexecute:\n  echo: true\n  eval: true\n  freeze: true\ncallout-appearance: minimal\n---\n\n## 1. Overview  \n* **K-Means (unsupervised):**  \n  The K-means algorithm was implemented from scratch, applied to the *Palmer Penguins* dataset using **bill length** and **flipper length**.  Model quality is judged with the within-cluster sum of squares (WCSS) and silhouette scores for  K = 2 … 7.  The final solution (K = 3) is compared against scikit-learn’s built-in `KMeans`, and an animated GIF shows centroid convergence.\n\n* **K-Nearest Neighbours (supervised):**  \n  A synthetic 2-dimensional, non-linear classification problem is generated with a wiggly sine boundary. K-NN is coded by hand and validated against scikit-learn’s `KNeighborsClassifier`. Test-set accuracy was plotted for  k = 1 … 30, highlight the optimal *k* (≈5), and display the resulting decision surface alongside the true boundary.\n\n---\n\n## 2. Load Data  \n\n::: {#78c94b68 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\npenguins = pd.read_csv(\"palmer_penguins.csv\")\npenguins.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181</td>\n      <td>3750</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186</td>\n      <td>3800</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195</td>\n      <td>3250</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193</td>\n      <td>3450</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.3</td>\n      <td>20.6</td>\n      <td>190</td>\n      <td>3650</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n## 3. K-Means (1a)\n\n### 3.1 Scale features  \n\n::: {#e972acc8 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nX_kmeans = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna()\nX_scaled = StandardScaler().fit_transform(X_kmeans)\n```\n:::\n\n\n### 3.2 Custom K-Means function  \n\n::: {#895afeba .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\ndef kmeans_custom(X, k, max_iter=100, seed=42):\n    rng = np.random.default_rng(seed)\n    centroids = X[rng.choice(len(X), k, replace=False)]\n    history   = [centroids.copy()]\n    for _ in range(max_iter):\n        dists   = np.linalg.norm(X[:,None,:] - centroids[None,:,:], axis=2)\n        labels  = dists.argmin(axis=1)\n        new_c   = np.array([X[labels==i].mean(0) for i in range(k)])\n        history.append(new_c.copy())\n        if np.allclose(centroids, new_c): break\n        centroids = new_c\n    return labels, centroids, history\n```\n:::\n\n\n### 3.3 WCSS & silhouette (K = 2–7)  \n\n::: {#a0cb9436 .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\n\nks, wcss, sil = [], [], []\nfor k in range(2,8):\n    lab, cen, _ = kmeans_custom(X_scaled, k)\n    ks.append(k)\n    wcss.append(((X_scaled - cen[lab])**2).sum())\n    sil.append(silhouette_score(X_scaled, lab))\n\nplt.plot(ks, wcss, marker=\"o\", label=\"WCSS\")\nplt.plot(ks, sil,  marker=\"s\", label=\"Silhouette\")\nplt.xlabel(\"k\"); plt.ylabel(\"Score\"); plt.legend()\nplt.title(\"WCSS & Silhouette vs k\"); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=593 height=449}\n:::\n:::\n\n\n### 3.4 Fit K = 3 & compare to sklearn  \n\n::: {#87ddaa8c .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nlabels_c, cents_c, hist = kmeans_custom(X_scaled, 3)\nkm = KMeans(n_clusters=3, n_init=10, random_state=42).fit(X_scaled)\nprint(\"Custom centroids:\\n\", cents_c, \"\\n\\nSklearn centroids:\\n\", km.cluster_centers_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCustom centroids:\n [[ 0.9235766  -0.37501448]\n [-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]] \n\nSklearn centroids:\n [[-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]\n [ 0.9235766  -0.37501448]]\n```\n:::\n:::\n\n\n### 3.5 Centroid-movement GIF  \n\n::: {#12a9a3b0 .cell execution_count=6}\n``` {.python .cell-code}\nfrom PIL import Image, ImageDraw\nframes=[]\nfor step,cent in enumerate(hist):\n    img=Image.new(\"RGB\",(400,400),\"white\"); dr=ImageDraw.Draw(img)\n    scaled=(X_scaled-X_scaled.min(0))/(X_scaled.max(0)-X_scaled.min(0))\n    for x,y in scaled: dr.ellipse([(x*380+10,y*380+10),(x*380+14,y*380+14)], fill=\"#888\")\n    for j,(cx,cy) in enumerate((cent-X_scaled.min(0))/(X_scaled.max(0)-X_scaled.min(0))):\n        pcx,pcy=(cx*380+10, cy*380+10)\n        dr.ellipse([(pcx-6,pcy-6),(pcx+6,pcy+6)], outline=\"red\", width=2)\n        dr.text((pcx+8,pcy-10),f\"C{j}\", fill=\"black\")\n    dr.text((10,10),f\"Step {step}\", fill=\"black\")\n    frames.append(img)\nframes[0].save(\"kmeans_animation.gif\", save_all=True,\n               append_images=frames[1:], duration=500, loop=0)\n```\n:::\n\n\n![GIF](kmeans_animation.gif){width=55%}\n\n### 3.6 Interpretation  \n\nThe elbow in WCSS and the highest silhouette score both occur at **K = 3**, matching the three natural bill-/flipper-length clusters visible in the GIF. Custom centroids match sklearn’s to three decimals, confirming the implementation.\n\n---\n\n## 4. K-Nearest Neighbours (2a)\n\n### 4.1 Training data (seed 42)  \n\n::: {#7d07aaa4 .cell execution_count=7}\n``` {.python .cell-code}\nnp.random.seed(42); n=100\nx1 = np.random.uniform(-3,3,n)\nx2 = np.random.uniform(-3,3,n)\nboundary = np.sin(4*x1)+x1\ny = (x2>boundary).astype(int)\ntrain = pd.DataFrame(dict(x1=x1,x2=x2,y=y))\n```\n:::\n\n\n### 4.2 Plot with boundary  \n\n::: {#b20414d6 .cell execution_count=8}\n``` {.python .cell-code}\nplt.scatter(train.x1,train.x2,c=train.y,cmap=\"bwr\",edgecolor=\"k\")\nxs = np.linspace(-3,3,400)\nplt.plot(xs, np.sin(4*xs)+xs, \"k--\", lw=1)\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.title(\"Training data + boundary\"); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=587 height=449}\n:::\n:::\n\n\n### 4.3 Test data (seed 2025)  \n\n::: {#98d213c1 .cell execution_count=9}\n``` {.python .cell-code}\nnp.random.seed(2025)\nx1t = np.random.uniform(-3,3,n); x2t = np.random.uniform(-3,3,n)\nboundary_t = np.sin(4*x1t)+x1t\ny_t = (x2t>boundary_t).astype(int)\ntest = pd.DataFrame(dict(x1=x1t,x2=x2t,y=y_t))\n```\n:::\n\n\n### 4.4 Hand-coded KNN + accuracy curve  \n\n::: {#8661df3c .cell execution_count=10}\n``` {.python .cell-code}\nfrom collections import Counter\ndef knn_predict(Xtr,ytr,Xtest,k):\n    preds=[]\n    for x in Xtest:\n        d=np.linalg.norm(Xtr-x,axis=1)\n        preds.append(Counter(ytr[d.argsort()[:k]]).most_common(1)[0][0])\n    return np.array(preds)\n\nXtr=train[[\"x1\",\"x2\"]].values; ytr=train.y.values\nXte=test[[\"x1\",\"x2\"]].values; yte=test.y.values\nacc=[]\nfor k in range(1,31):\n    acc.append((knn_predict(Xtr,ytr,Xte,k)==yte).mean())\n\nbest_k = np.argmax(acc)+1\nplt.plot(range(1,31),acc,marker=\"o\"); plt.axvline(best_k,color=\"red\",ls=\"--\")\nplt.xlabel(\"k\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy vs k\"); plt.show()\nprint(\"Best k =\", best_k)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=597 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBest k = 5\n```\n:::\n:::\n\n\n### 4.5 Sklearn check  \n\n::: {#38bbd9c2 .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=best_k).fit(Xtr, ytr)\nprint(\"Sklearn accuracy:\", clf.score(Xte, yte))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSklearn accuracy: 0.95\n```\n:::\n:::\n\n\n### 4.6 Decision surface  \n\n::: {#2393d88c .cell execution_count=12}\n``` {.python .cell-code}\nxx,yy=np.meshgrid(np.linspace(-3,3,400),np.linspace(-3,3,400))\nZ=knn_predict(Xtr,ytr,np.c_[xx.ravel(),yy.ravel()],best_k).reshape(xx.shape)\nplt.contourf(xx,yy,Z,alpha=0.3,cmap=\"bwr\")\nplt.scatter(test.x1,test.x2,c=test.y,cmap=\"bwr\",edgecolor=\"k\",s=40)\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.title(f\"Decision surface (k={best_k})\"); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=592 height=449}\n:::\n:::\n\n\n### 4.7 Interpretation  \n\nAccuracy peaks at **k = 5 (~95 %)**: small k overfits noise; large k oversmooths. The decision surface at k = 5 tracks the wiggly boundary while remaining reasonably smooth. Sklearn replicates the same accuracy, validating the hand algorithm.\n\n---\n\n## 5. Discussion  \n\n* **K-Means:** Both WCSS elbow and silhouette concur on K = 3. Animated convergence shows stable centroids in < 10 iterations.  \n* **K-NN:** Optimal k balances variance and bias; visual boundary confirms reliable classification.  \n* **Take-away:** From-scratch implementations deepen intuition; built-ins quickly confirm correctness.\n\n---\n\n## 6. Full Code\n\n::: {#e036c734 .cell execution_count=13}\n``` {.python .cell-code}\n# Imports\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom collections import Counter\nfrom PIL import Image, ImageDraw\n\n# K-Means \npenguins = pd.read_csv(\"palmer_penguins.csv\")\nX = StandardScaler().fit_transform(penguins[[\"bill_length_mm\",\"flipper_length_mm\"]].dropna())\n\ndef kmeans_custom(X,k,max_iter=100,seed=42):\n    rng=np.random.default_rng(seed)\n    c=X[rng.choice(len(X),k,False)]; history=[c.copy()]\n    for _ in range(max_iter):\n        d=np.linalg.norm(X[:,None,:]-c[None,:,:],axis=2); lab=d.argmin(1)\n        n=np.array([X[lab==i].mean(0) for i in range(k)]); history.append(n.copy())\n        if np.allclose(c,n): break; c=n\n    return lab,c,history\n\nlab,c,hist=kmeans_custom(X,3)\nprint(\"Custom centroids:\",c)\nprint(\"Sklearn centroids:\",\n      KMeans(n_clusters=3,n_init=10,random_state=42).fit(X).cluster_centers_)\n\n# K-NN \ndef gen(seed):\n    np.random.seed(seed); n=100\n    x1=np.random.uniform(-3,3,n); x2=np.random.uniform(-3,3,n)\n    y=(x2>np.sin(4*x1)+x1).astype(int)\n    return pd.DataFrame(dict(x1=x1,x2=x2,y=y))\n\ntrain=gen(42); test=gen(2025)\nXtr,ytr=train[[\"x1\",\"x2\"]].values,train.y.values\nXte,yte=test[[\"x1\",\"x2\"]].values,test.y.values\n\ndef knn(Xtr,ytr,Xte,k):\n    out=[]\n    for x in Xte:\n        d=np.linalg.norm(Xtr-x,axis=1)\n        out.append(Counter(ytr[d.argsort()[:k]]).most_common(1)[0][0])\n    return np.array(out)\n\nacc=[(knn(Xtr,ytr,Xte,k)==yte).mean() for k in range(1,31)]\nbest_k=np.argmax(acc)+1\nprint(\"Best k:\",best_k,\" Custom acc:\",acc[best_k-1],\n      \" Sklearn acc:\",\n      KNeighborsClassifier(n_neighbors=best_k).fit(Xtr,ytr).score(Xte,yte))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCustom centroids: [[ 0.45915754  1.14564997]\n [-1.39050655 -0.42637319]\n [ 1.37483282  2.07457274]]\nSklearn centroids: [[-0.96427851 -0.80518575]\n [ 0.66403058  1.15087842]\n [ 0.9235766  -0.37501448]]\nBest k: 5  Custom acc: 0.95  Sklearn acc: 0.95\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}