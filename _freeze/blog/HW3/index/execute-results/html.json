{
  "hash": "1da936c679d844c91c77151bec2d4a7a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multinomial Logit Model\"\nauthor: \"Idris Huang\"\ndate: May 28, 2025\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show/Hide Code\"\n    toc: true\n    toc-depth: 2\nexecute:\n  echo: true\n  eval: true\n  freeze: true\ncallout-appearance: minimal\n---\n\nThis assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n\n## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n\n\n## 2. Simulate Conjoint Data\n\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n\n$$\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n$$\n\nwhere the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n\nThe following code imports the previously–simulated conjoint data.\n\n::: {#2e98d95f .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nconjoint = pd.read_csv(\"conjoint_data.csv\")\nconjoint.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>resp</th>\n      <th>task</th>\n      <th>choice</th>\n      <th>brand</th>\n      <th>ad</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>N</td>\n      <td>Yes</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>H</td>\n      <td>Yes</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>P</td>\n      <td>Yes</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>N</td>\n      <td>Yes</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>P</td>\n      <td>Yes</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## 3. Preparing the Data for Estimation\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n::: {#9a34eefe .cell execution_count=2}\n``` {.python .cell-code}\n# Data preparation for MNL\nimport pandas as pd\nimport numpy as np\n\nX = (\n    conjoint\n    .assign(\n        brand_N=lambda d: (d[\"brand\"] == \"N\").astype(int),\n        brand_P=lambda d: (d[\"brand\"] == \"P\").astype(int),\n        ad_yes=lambda d: (d[\"ad\"] == \"Yes\").astype(int)\n    )\n)\ndesign_cols = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]\ndesign_cols_mle   = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]       \ndesign_cols_bayes = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price_coef\"]  \nX_design = X[design_cols].to_numpy()\ny = X[\"choice\"].to_numpy()\ntasks = (X[\"resp\"].astype(str) + \"_\" + X[\"task\"].astype(str)).to_numpy()\n\nprint(\"Design matrix shape:\", X_design.shape)\nprint(\"Number of unique choice tasks:\", len(np.unique(tasks)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDesign matrix shape: (3000, 4)\nNumber of unique choice tasks: 1000\n```\n:::\n:::\n\n\n## 4. Estimation via Maximum Likelihood\n\n::: {#8a398ef5 .cell execution_count=3}\n``` {.python .cell-code}\n# Maximum Likelihood Estimation\nimport numpy as np\nfrom scipy.optimize import minimize\nimport pandas as pd\n\ndef neg_loglike(beta):\n    ll = 0.0\n    for t in np.unique(tasks):\n        idx = tasks == t\n        utilities = X_design[idx] @ beta\n        expu = np.exp(utilities)\n        probs = expu / expu.sum()\n        ll += np.log(probs[y[idx] == 1][0])\n    return -ll\n\ninit_beta = np.zeros(X_design.shape[1])\nres = minimize(neg_loglike, init_beta, method=\"BFGS\")\nbeta_hat = res.x\ncov = res.hess_inv\nse = np.sqrt(np.diag(cov))\n\nmle_results = pd.DataFrame({\n    \"coef\": beta_hat,\n    \"std_err\": se,\n    \"z\": beta_hat / se\n}, index=design_cols_mle)\n\nmle_results\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std_err</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>brand_N</th>\n      <td>0.941195</td>\n      <td>0.063014</td>\n      <td>14.936269</td>\n    </tr>\n    <tr>\n      <th>brand_P</th>\n      <td>0.501616</td>\n      <td>0.026838</td>\n      <td>18.690485</td>\n    </tr>\n    <tr>\n      <th>ad_yes</th>\n      <td>-0.731994</td>\n      <td>0.013983</td>\n      <td>-52.350695</td>\n    </tr>\n    <tr>\n      <th>price</th>\n      <td>-0.099480</td>\n      <td>0.008785</td>\n      <td>-11.323711</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Interpretation of MLE estimates\n\n* **All coefficients are highly significant** (|z| ≫ 1.96) and have the expected signs.  \n* **Brand effects:** Choosing **Netflix** (β ≈ 0.94) raises utility almost twice as much as choosing **Amazon Prime** (β ≈ 0.50) relative to Hulu.  Converting to willingness-to-pay with –β<sub>price</sub> (≈ 0.10) gives ≈ \\$9.5 / month for Netflix and ≈ \\$5.0 for Prime.  \n* **Advertising penalty:** An ad-supported plan lowers utility by −0.73, implying consumers need about a **\\$7.4/month discount** to accept ads—close to real-world pricing for “with-ads” tiers.  \n* **Price sensitivity:** Utility falls by ≈ 0.10 for every extra \\$1 of monthly fee—price matters, but less than brand or ads.  \n* 95 % confidence intervals (β ± 1.96 × SE) exclude zero for every attribute, confirming statistical significance.\n\n## 5. Estimation via Bayesian Methods\n\n::: {#b632853b .cell execution_count=4}\n``` {.python .cell-code}\n# Bayesian Estimation via MCMC\nimport numpy as np\nimport pymc as pm\nimport arviz as az\n\nwith pm.Model() as mnl_bayes:\n    # Priors\n    brand_N = pm.Normal(\"brand_N\", mu=0, sigma=5)\n    brand_P = pm.Normal(\"brand_P\", mu=0, sigma=5)\n    ad_yes = pm.Normal(\"ad_yes\", mu=0, sigma=5)\n    price_coef = pm.Normal(\"price_coef\", mu=0, sigma=1)\n\n    beta_vec = pm.math.stack([brand_N, brand_P, ad_yes, price_coef])\n\n    # Utilities and choice probabilities\n    utilities = pm.math.dot(X_design, beta_vec)\n    J = 3  # alternatives per task\n    utilities_task = utilities.reshape((-1, J))\n    p = pm.math.softmax(utilities_task, axis=1)\n\n    # Observed choices (index of chosen alt per task)\n    y_choice = y.reshape((-1, J)).argmax(axis=1)\n    pm.Categorical(\"choice\", p=p, observed=y_choice)\n\n    trace = pm.sample(draws=10_000, tune=1_000,\n                      target_accept=0.9,\n                      return_inferencedata=True,\n                      progressbar=False)\n\naz.plot_trace(trace, var_names=design_cols_bayes)\naz.summary(trace, var_names=design_cols_bayes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [brand_N, brand_P, ad_yes, price_coef]\nSampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 8 seconds.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>brand_N</th>\n      <td>0.943</td>\n      <td>0.111</td>\n      <td>0.735</td>\n      <td>1.155</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>32820.0</td>\n      <td>29291.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>brand_P</th>\n      <td>0.502</td>\n      <td>0.111</td>\n      <td>0.294</td>\n      <td>0.711</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>32313.0</td>\n      <td>29139.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>ad_yes</th>\n      <td>-0.734</td>\n      <td>0.088</td>\n      <td>-0.900</td>\n      <td>-0.570</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>36807.0</td>\n      <td>29491.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>price_coef</th>\n      <td>-0.100</td>\n      <td>0.006</td>\n      <td>-0.112</td>\n      <td>-0.088</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>35236.0</td>\n      <td>29361.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-3.png){}\n:::\n:::\n\n\n### Posterior diagnostics and comparison to MLE\n\nPosterior means (0.94, 0.50, −0.74, −0.10) are virtually identical to the MLE estimates, confirming that weakly-informative priors let the data dominate.\n\n* **Credible intervals:** 94 % HDIs comfortably straddle the true simulated values and exclude zero.  \n* **Chain quality:** Effective sample sizes > 30 k and $\\hat R = 1.00$ for every parameter; trace plots show well-mixed, stationary chains with no divergences.  \n* **Bayesian WTP:** Converting coefficients yields WTP<sub>Netflix</sub> ≈ \\$9.4, WTP<sub>Prime</sub> ≈ \\$5.0, and a –\\$7.3 penalty for ads—nearly identical to MLE numbers but now accompanied by full uncertainty bounds.\n\n## 6. Discussion\n\n**Managerial take-aways**\n\n1. **Brand still rules:** A \\$4–10 monthly premium for the Netflix label underscores the enduring power of brand equity.  \n2. **Ads need real discounts:** Consumers demand roughly a **\\$7/month** price break to tolerate ads; smaller gaps risk churn or dissatisfaction.  \n3. **Moderate price elasticity:** A \\$1 hike lowers utility by ~0.10—enough to shift share, but weaker than brand or ad effects.  \n4. **MLE vs. Bayesian:** With large, balanced conjoint data the two approaches converge; Bayesian estimation adds credible intervals and an easy path to hierarchical models.  \n5. **Potential next steps:**  \n   * Fit a hierarchical (HB-MNL) model to capture heterogeneity in price and ad sensitivity.  \n   * Include a “no-choice” option and more brands for greater external validity.  \n   * Validate conjoint-based WTP against actual subscription behavior.\n\n## 7. Full Code\n\n::: {#ee936062 .cell execution_count=5}\n``` {.python .cell-code}\n# Imports \nimport pandas as pd\nimport numpy as np\nimport pymc as pm\nimport arviz as az\nfrom scipy.optimize import minimize\n\n# Load Data\nconjoint = pd.read_csv(\"conjoint_data.csv\")\nconjoint = (\n    conjoint\n    .assign(\n        brand_N=lambda d: (d[\"brand\"] == \"N\").astype(int),\n        brand_P=lambda d: (d[\"brand\"] == \"P\").astype(int),\n        ad_yes=lambda d: (d[\"ad\"] == \"Yes\").astype(int)\n    )\n)\ndesign_cols = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]\ndesign_cols_mle   = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]       \ndesign_cols_bayes = [\"brand_N\", \"brand_P\", \"ad_yes\", \"price_coef\"]  \nX_design = conjoint[design_cols].to_numpy()\ny = conjoint[\"choice\"].to_numpy()\ntasks = (conjoint[\"resp\"].astype(str) + \"_\" + conjoint[\"task\"].astype(str)).to_numpy()\nJ = 3  # alternatives per task\n\n# Maximum Likelihood \ndef neg_loglike(beta):\n    ll = 0.0\n    for t in np.unique(tasks):\n        idx = tasks == t\n        utilities = X_design[idx] @ beta\n        p = np.exp(utilities)\n        p /= p.sum()\n        ll += np.log(p[y[idx]==1][0])\n    return -ll\n\nbeta0 = np.zeros(X_design.shape[1])\nres = minimize(neg_loglike, beta0, method=\"BFGS\")\nbeta_hat = res.x\ncov = res.hess_inv\nse = np.sqrt(np.diag(cov))\n\nmle_results = pd.DataFrame({\n    \"coef\": beta_hat,\n    \"std_err\": se,\n    \"z\": beta_hat/se\n}, index=design_cols_mle)\n\nprint(\"MLE results:\")\nprint(mle_results)\n\n# Bayesian Estimation\nwith pm.Model() as mnl_bayes:\n    brand_N = pm.Normal(\"brand_N\", mu=0, sigma=5)\n    brand_P = pm.Normal(\"brand_P\", mu=0, sigma=5)\n    ad_yes = pm.Normal(\"ad_yes\", mu=0, sigma=5)\n    price_coef = pm.Normal(\"price_coef\", mu=0, sigma=1)\n    beta_vec = pm.math.stack([brand_N, brand_P, ad_yes, price_coef])\n    utilities = pm.math.dot(X_design, beta_vec)\n    utilities_task = utilities.reshape((-1,J))\n    p = pm.math.softmax(utilities_task, axis=1)\n    y_choice = y.reshape((-1,J)).argmax(axis=1)\n    pm.Categorical(\"choice\", p=p, observed=y_choice)\n    trace = pm.sample(draws=10_000, tune=1_000, target_accept=0.9,\n                      return_inferencedata=True)\n\nprint(az.summary(trace, var_names=design_cols_bayes))\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}